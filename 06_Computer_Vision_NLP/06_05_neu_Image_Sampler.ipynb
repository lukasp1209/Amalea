{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_05_neu: Image Sampler & Quick CV Lab\n",
    "\n",
    "Ziele:\n",
    "- Bestehende Demo-Bilder nutzen (oder Fallback-Synthetics)\n",
    "- Schneller Image-Browser ohne Training\n",
    "- Edge/Contour/ORB-Pipeline aus den neuen Notebooks wiederverwenden\n",
    "- Einfache Augmentations als visuelle Vorschau\n",
    "- CPU-freundlich, alles lokal/offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warum dieses Notebook?\n",
    "- Mini-Story: Du bereitest Bilder für ein kleines Edge-Detection-PoC vor und musst schnell prüfen, welche Schritte (Canny/Contours/ORB) auf deinem Datenstapel funktionieren.\n",
    "- Ziel: Ohne Training sofort sehen, was deine Pipeline mit echten Assets macht, und schnelle Parameter-Iterationen fahren.\n",
    "- Entscheidungshilfe: Wann Canny? (saubere Kanten), Wann Contours? (Objekt-Umrisse zählen), Wann ORB? (Keypoints/Matching, patentfrei)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b3cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV 4.12.0\n"
     ]
    }
   ],
   "source": [
    "# Imports & Setup\n",
    "import os, glob, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ipywidgets import interact, Dropdown, IntSlider, FloatSlider, Checkbox, fixed\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "print(\"OpenCV\", cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4cfbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 image(s) under images/; defaulting to synthetic if empty.\n"
     ]
    }
   ],
   "source": [
    "# Helpers: list and load images\n",
    "\n",
    "def list_images(root=\"images\"):\n",
    "    patterns = [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.bmp\", \"*.gif\"]\n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files.extend(glob.glob(os.path.join(root, \"**\", pat), recursive=True))\n",
    "    files = [os.path.relpath(f, root) for f in files]\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def load_image(path=None, size=(400, 400), root=\"images\"):\n",
    "    if path and path != \"synthetic_stripes\":\n",
    "        full = os.path.join(root, path)\n",
    "        if os.path.exists(full):\n",
    "            img = cv2.cvtColor(cv2.imread(full), cv2.COLOR_BGR2RGB)\n",
    "            if size:\n",
    "                img = cv2.resize(img, size)\n",
    "        else:\n",
    "            print(f\"Missing file: {full}, using synthetic instead\")\n",
    "            img = None\n",
    "    else:\n",
    "        img = None\n",
    "    if img is None:\n",
    "        xs = np.linspace(0, 1, size[0])\n",
    "        ys = np.linspace(0, 1, size[1])\n",
    "        xv, yv = np.meshgrid(xs, ys)\n",
    "        img = np.stack([\n",
    "            (xv * 255).astype(np.uint8),\n",
    "            (yv * 255).astype(np.uint8),\n",
    "            ((xv > 0.5) ^ (yv > 0.5)).astype(np.uint8) * 255,\n",
    "        ], axis=-1)\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "images = list_images()\n",
    "options = [\"synthetic_stripes\"] + images\n",
    "print(f\"Found {len(images)} image(s) under images/; defaulting to synthetic if empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f155a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc29619339bb4bfa9e791fc16d82ada4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='image', options=('synthetic_stripes', 'Faltung1.png', 'Faltung2.pn…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Browse a sample image\n",
    "\n",
    "@interact(img=Dropdown(options=options, description=\"image\"))\n",
    "def show_image(img=\"synthetic_stripes\"):\n",
    "    arr = load_image(img)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(arr)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "955c292e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f671efd86a144352a2ed562de3122347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='image', options=('synthetic_stripes', 'Faltung1.png', 'Faltung2.pn…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Edge detectors (reuse helpers)\n",
    "\n",
    "def to_gray_u8(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if img.ndim == 3 else img\n",
    "    if gray.dtype != np.uint8:\n",
    "        gray = np.clip(gray, 0, 255).astype(np.uint8)\n",
    "    return gray\n",
    "\n",
    "\n",
    "def run_edges(img, low=80, high=150, ksize=3):\n",
    "    gray = to_gray_u8(img)\n",
    "    canny = cv2.Canny(gray, low, high)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=ksize)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=ksize)\n",
    "    mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    mag = np.clip(mag / (mag.max() + 1e-6) * 255, 0, 255).astype(np.uint8)\n",
    "    lap = cv2.Laplacian(gray, cv2.CV_32F)\n",
    "    lap = np.clip((lap - lap.min()) / (lap.max() - lap.min() + 1e-6) * 255, 0, 255).astype(np.uint8)\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    for ax, img_, title in zip(axes, [gray, canny, mag, lap], [\"Gray\", \"Canny\", \"Sobel mag\", \"Laplacian\"]):\n",
    "        ax.imshow(img_, cmap=\"gray\")\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@interact(\n",
    "    img=Dropdown(options=options, description=\"image\"),\n",
    "    low=IntSlider(80, 0, 200, 10, description=\"low\"),\n",
    "    high=IntSlider(150, 50, 300, 10, description=\"high\"),\n",
    "    ksize=IntSlider(3, 3, 7, 2, description=\"ksize\")\n",
    ")\n",
    "def _tune_edges(img=\"synthetic_stripes\", low=80, high=150, ksize=3):\n",
    "    if high <= low:\n",
    "        high = low + 10\n",
    "    arr = load_image(img)\n",
    "    run_edges(arr, low, high, int(ksize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e631c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b602bc4ee41e45a68c4767a6081fa076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='image', options=('synthetic_stripes', 'Faltung1.png', 'Faltung2.pn…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contours\n",
    "\n",
    "def detect_contours(img, thresh1=80, thresh2=150):\n",
    "    gray = to_gray_u8(img)\n",
    "    edges = cv2.Canny(gray, thresh1, thresh2)\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    result = img.copy()\n",
    "    cv2.drawContours(result, contours, -1, (255, 0, 0), 2)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(edges, cmap=\"gray\")\n",
    "    plt.title(\"Canny\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(result)\n",
    "    plt.title(f\"Contours: {len(contours)}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return contours\n",
    "\n",
    "\n",
    "@interact(\n",
    "    img=Dropdown(options=options, description=\"image\"),\n",
    "    thresh1=IntSlider(80, 0, 200, 10, description=\"th1\"),\n",
    "    thresh2=IntSlider(150, 50, 300, 10, description=\"th2\"),\n",
    ")\n",
    "def _tune_contours(img=\"synthetic_stripes\", thresh1=80, thresh2=150):\n",
    "    if thresh2 <= thresh1:\n",
    "        thresh2 = thresh1 + 10\n",
    "    arr = load_image(img)\n",
    "    detect_contours(arr, thresh1, thresh2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2fc107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c4a0c851e14f7b83561bd9f271ddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='image', options=('synthetic_stripes', 'Faltung1.png', 'Faltung2.pn…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ORB keypoints\n",
    "\n",
    "def run_orb(img, n_features=300):\n",
    "    orb = cv2.ORB_create(nfeatures=n_features)\n",
    "    gray = to_gray_u8(img)\n",
    "    kps, desc = orb.detectAndCompute(gray, None)\n",
    "    vis = cv2.drawKeypoints(img, kps, None, color=(0, 255, 0), flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(vis)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"ORB keypoints: {len(kps)}\")\n",
    "    plt.show()\n",
    "    return kps, desc\n",
    "\n",
    "\n",
    "@interact(\n",
    "    img=Dropdown(options=options, description=\"image\"),\n",
    "    n_features=IntSlider(200, 50, 500, 50, description=\"features\"),\n",
    ")\n",
    "def _tune_orb(img=\"synthetic_stripes\", n_features=200):\n",
    "    arr = load_image(img)\n",
    "    run_orb(arr, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7077af65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_Float.__init__() takes from 1 to 2 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     14\u001b[0m     aug \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mconvertScaleAbs(aug, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(contrast), beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(brightness))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aug\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@interact\u001b[39m(\n\u001b[1;32m     19\u001b[0m     img\u001b[38;5;241m=\u001b[39mDropdown(options\u001b[38;5;241m=\u001b[39moptions, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     20\u001b[0m     flip_h\u001b[38;5;241m=\u001b[39mCheckbox(\u001b[38;5;28;01mFalse\u001b[39;00m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflip_h\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     flip_v\u001b[38;5;241m=\u001b[39mCheckbox(\u001b[38;5;28;01mFalse\u001b[39;00m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflip_v\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     22\u001b[0m     rotate_deg\u001b[38;5;241m=\u001b[39mIntSlider(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m1\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     23\u001b[0m     brightness\u001b[38;5;241m=\u001b[39mIntSlider(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m5\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbright\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m---> 24\u001b[0m     contrast\u001b[38;5;241m=\u001b[39m\u001b[43mFloatSlider\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontrast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preview_aug\u001b[39m(img\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthetic_stripes\u001b[39m\u001b[38;5;124m\"\u001b[39m, flip_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, flip_v\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rotate_deg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, brightness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, contrast\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     27\u001b[0m     arr \u001b[38;5;241m=\u001b[39m load_image(img)\n\u001b[1;32m     28\u001b[0m     aug \u001b[38;5;241m=\u001b[39m augment(arr, flip_h, flip_v, rotate_deg, brightness, contrast)\n",
      "\u001b[0;31mTypeError\u001b[0m: _Float.__init__() takes from 1 to 2 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "# Simple augmentation preview (no training)\n",
    "\n",
    "def augment(img, flip_h=False, flip_v=False, rotate_deg=0, brightness=0, contrast=1.0):\n",
    "    aug = img.copy()\n",
    "    if flip_h:\n",
    "        aug = cv2.flip(aug, 1)\n",
    "    if flip_v:\n",
    "        aug = cv2.flip(aug, 0)\n",
    "    if rotate_deg != 0:\n",
    "        h, w = aug.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, rotate_deg, 1.0)\n",
    "        aug = cv2.warpAffine(aug, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    aug = cv2.convertScaleAbs(aug, alpha=float(contrast), beta=float(brightness))\n",
    "    return aug\n",
    "\n",
    "\n",
    "@interact(\n",
    "    img=Dropdown(options=options, description=\"image\"),\n",
    "    flip_h=Checkbox(False, description=\"flip_h\"),\n",
    "    flip_v=Checkbox(False, description=\"flip_v\"),\n",
    "    rotate_deg=IntSlider(0, -25, 25, 1, description=\"rotate\"),\n",
    "    brightness=IntSlider(0, -50, 50, 5, description=\"bright\"),\n",
    "    contrast=FloatSlider(value=1.0, min=0.5, max=1.5, step=0.05, description=\"contrast\"),\n",
    ")\n",
    "def _preview_aug(img=\"synthetic_stripes\", flip_h=False, flip_v=False, rotate_deg=0, brightness=0, contrast=1.0):\n",
    "    arr = load_image(img)\n",
    "    aug = augment(arr, flip_h, flip_v, rotate_deg, brightness, contrast)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(arr)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(aug)\n",
    "    plt.title(\"Augmented\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipps\n",
    "- Läuft komplett offline; wenn `images/` fehlt, werden synthetische Streifen genutzt.\n",
    "- Für schnelle Läufe Bildgröße bei Bedarf in `load_image(size=...)` weiter verkleinern.\n",
    "- Die Widgets nutzen dieselben Kern-Schritte wie in den neuen Notebooks, aber ohne Training/Weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selbstcheck\n",
    "- Welche drei Schritte bietet das Notebook an und wann würdest du jeden verwenden?\n",
    "- Was passiert, wenn `images/` fehlt?\n",
    "- Wie veränderst du die Performance auf einer langsamen CPU?\n",
    "\n",
    "## Troubleshooting\n",
    "- Widgets rendern nicht: Stelle sicher, dass `ipywidgets` installiert/aktiviert ist und Kernel neu starten.\n",
    "- Canny/Contours leer: Schwellenwerte senken oder Bildgröße reduzieren.\n",
    "- ORB liefert wenige Keypoints: `n_features` erhöhen und auf Graustufen achten (kontrastreiche Bilder).\n",
    "- Augmentation verzerrt: Rotationswinkel kleiner wählen oder Border-Mode auf REFLECT lassen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
