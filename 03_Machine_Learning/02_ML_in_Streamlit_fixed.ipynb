{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9befe4d2",
   "metadata": {},
   "source": [
    "# ü§ñ Woche 3: Machine Learning Engineering\n",
    " \n",
    "> üöÄ **Motivation f√ºr Entwickler:**\n",
    "> \n",
    "> **Software 1.0:** Du schreibst Regeln (Code), um Daten zu verarbeiten.  \n",
    "> **Software 2.0 (ML):** Du f√ºtterst Daten und Antworten in einen Algorithmus, und der **findet die Regeln selbst**.\n",
    "> \n",
    "> In dieser Woche bauen wir keine \"Skripte\", sondern **trainierbare Systeme**, die wir in Streamlit deployen.\n",
    " \n",
    "## üìö Der Tech-Stack\n",
    "- **Scikit-Learn:** Die Standard-Bibliothek f√ºr klassisches ML (wie `java.util` f√ºr Java).\n",
    "- **Streamlit:** Unser Frontend f√ºr die Modelle.\n",
    "- **Joblib:** Zum Speichern (Serialisieren) von trainierten Modellen.\n",
    " \n",
    "## üó∫Ô∏è Roadmap\n",
    " \n",
    "- **ML-Grundlagen** aus dem urspr√ºnglichen AMALEA-Kurs verstehen\n",
    "- **Deskriptive vs. Pr√§diktive Statistik** (AMALEA-Konzept)\n",
    "- **Training/Test/Validation** Datenaufteilung (AMALEA-Standard)\n",
    "- **ML in Streamlit** integrieren\n",
    "- **Iris-Datensatz** klassifizieren (AMALEA-Klassiker)\n",
    "- **Interaktive ML-Apps** erstellen\n",
    " \n",
    "---\n",
    " \n",
    "## üß† ML-Grundlagen aus dem urspr√ºnglichen AMALEA-Kurs\n",
    " \n",
    "### Definition aus dem urspr√ºnglichen Kurs:\n",
    " \n",
    "> *\"Machine Learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world.\"* -- Nvidia\n",
    " \n",
    "### Wichtige Konzepte:\n",
    " \n",
    "**üìä Deskriptive Statistik** (Vergangenheit verstehen)\n",
    "- Daten aus vergangenen Ereignissen analysieren\n",
    "- Grundlegende Techniken: Anzahl, Summe, Durchschnitt, Min/Max\n",
    "- **Ziel**: Was ist passiert?\n",
    " \n",
    "**üîÆ Pr√§diktive Statistik** (Zukunft vorhersagen)\n",
    "- Basierend auf historischen Daten Vorhersagen treffen\n",
    "- Verwendet Modelle und Algorithmen\n",
    "- **Ziel**: Was wird passieren?\n",
    " \n",
    "**ü§ñ Machine Learning**\n",
    "- Automatisierter Ansatz f√ºr pr√§diktive Statistik\n",
    "- Lernt Muster aus Daten statt explizite Regeln\n",
    "- **Ziel**: Bessere Vorhersagen als regelbasierte Systeme\n",
    " \n",
    "### Datenaufteilung (AMALEA-Standard):\n",
    " \n",
    "| Datensatz | Zweck | Anteil |\n",
    "|-----------|-------|--------|\n",
    "| **Training** | Modell-Parameter optimieren | ~60-80% |\n",
    "| **Validation** | Hyperparameter tuning | ~10-20% |\n",
    "| **Test** | Finale Bewertung (nur einmal!) | ~10-20% |\n",
    " \n",
    "> **Wichtig**: Teste nie mit den Trainingsdaten! Das w√§re Betrug!\n",
    " \n",
    "---\n",
    " \n",
    "## Lernziele dieser Woche\n",
    "- Einfache ML-Modelle in Streamlit integrieren\n",
    "- Benutzer-Eingaben f√ºr Vorhersagen verwenden\n",
    "- Model Training und Evaluation in der App\n",
    "- Interaktive ML-Demos erstellen\n",
    " \n",
    "## Von Datenanalyse zu Vorhersagen\n",
    "Letzte Woche hast du gelernt, Daten zu visualisieren. Diese Woche machst du den n√§chsten Schritt: Du l√§sst die App Vorhersagen treffen!\n",
    " \n",
    "### üß† Was ist Machine Learning?\n",
    "**Machine Learning (ML)** = Computer lernen Muster aus Daten und machen Vorhersagen\n",
    " \n",
    "**Einfaches Beispiel:**\n",
    "- **Daten:** Gr√∂√üe und Gewicht von 1000 Menschen\n",
    "- **Muster:** Gr√∂√üere Menschen wiegen meist mehr\n",
    "- **Vorhersage:** Bei neuer Person mit Gr√∂√üe 180cm ‚Üí sch√§tze Gewicht\n",
    " \n",
    "**Haupttypen:**\n",
    "- **Klassifikation:** Kategorie vorhersagen (z.B. Spam/Nicht-Spam)\n",
    "- **Regression:** Zahlenwert vorhersagen (z.B. Hauspreise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3f5de",
   "metadata": {},
   "source": [
    "## üì¶ Zus√§tzliche Pakete f√ºr ML\n",
    "\n",
    "**Scikit-learn** = Die wichtigste ML-Bibliothek f√ºr Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ef39a",
   "metadata": {},
   "source": [
    "## üé¨ Erg√§nzende Videos: Machine Learning Grundlagen\n",
    "\n",
    "**üìº Original AMALEA Video-Serie (KIT 2021):**\n",
    "\n",
    "### Video 1: Maschinelles Lernen und seine Anwendungen  \n",
    "`Kurs-Videos/amalea-kit2021-w2v1 (1080p).mp4` (~25 min)\n",
    "- Was ist Machine Learning?\n",
    "- Supervised vs. Unsupervised Learning\n",
    "- Real-World Anwendungen\n",
    "\n",
    "### Video 2: 100% Genauigkeit - Das muss doch gut sein, oder?  \n",
    "`Kurs-Videos/amalea-kit2021-w2v2 (1080p).mp4` (~20 min)\n",
    "- Overfitting vs. Underfitting\n",
    "- Training vs. Validation vs. Test Sets\n",
    "- Warum 100% Accuracy oft schlecht ist\n",
    "\n",
    "### Video 3: Oh sorry, das war ein Falsch-Positiv  \n",
    "`Kurs-Videos/amalea-kit2021-w2v3 (1080p).mp4` (~30 min)\n",
    "- Confusion Matrix verstehen\n",
    "- Precision, Recall, F1-Score\n",
    "- ROC Curves und AUC\n",
    "\n",
    "üí° **Empfehlung:** Schau dir die Videos an, bevor du die ML-Algorithmen in Streamlit implementierst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e37a333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "streamlit==1.52.1\n",
    "pandas==2.2.3\n",
    "plotly==5.24.1\n",
    "numpy==1.26.4\n",
    "scikit-learn==1.6.0\n",
    "joblib==1.4.2\n",
    "matplotlib==3.10.6\n",
    "seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e37a333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit==1.52.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.52.1)\n",
      "Requirement already satisfied: pandas==2.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: plotly==5.24.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (5.24.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn==1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.6.0)\n",
      "Collecting joblib==1.4.2 (from -r requirements.txt (line 6))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: matplotlib==3.10.6 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (3.10.6)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (4.2.2)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (8.2.1)\n",
      "Requirement already satisfied: packaging>=20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (4.25.8)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (8.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.52.1->-r requirements.txt (line 1)) (6.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn==1.6.0->-r requirements.txt (line 5)) (1.16.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn==1.6.0->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 7)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 7)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 7)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: entrypoints in /opt/anaconda3/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (0.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (4.25.0)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.52.1->-r requirements.txt (line 1)) (4.0.12)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.52.1->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.52.1->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.52.1->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.52.1->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.52.1->-r requirements.txt (line 1)) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.1->-r requirements.txt (line 1)) (0.22.3)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.5.1\n",
      "    Uninstalling joblib-1.5.1:\n",
      "      Successfully uninstalled joblib-1.5.1\n",
      "Successfully installed joblib-1.4.2\n",
      "‚úÖ ML-Pakete erfolgreich installiert!\n",
      "ü§ñ Machine Learning Grundlagen - AMALEA modernisiert\n",
      "============================================================\n",
      "1Ô∏è‚É£ Iris-Datensatz laden (bekannt aus dem urspr√ºnglichen AMALEA-Kurs):\n",
      "üìä Datensatz-Info:\n",
      "- Samples (Zeilen): 150\n",
      "- Features (Spalten): 4\n",
      "- Klassen: ['setosa', 'versicolor', 'virginica']\n",
      "\n",
      "üìã Erste 5 Zeilen:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n",
      "\n",
      "2Ô∏è‚É£ Features vs. Target identifizieren (AMALEA-Konzept):\n",
      "Features (Input): ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target (Output): Species ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "3Ô∏è‚É£ Datenaufteilung nach AMALEA-Standard:\n",
      "Training: 120 Samples (80%)\n",
      "Test: 30 Samples (20%)\n",
      "\n",
      "4Ô∏è‚É£ Random Forest Algorithmus (aus AMALEA-Kurs):\n",
      "\n",
      "5Ô∏è‚É£ Vorhersagen auf Testdaten:\n",
      "Genauigkeit: 90.00%\n",
      "\n",
      "6Ô∏è‚É£ Feature Importance (Welche Merkmale sind wichtig?):\n",
      "- sepal length (cm): 0.116\n",
      "- sepal width (cm): 0.015\n",
      "- petal length (cm): 0.431\n",
      "- petal width (cm): 0.437\n",
      "\n",
      "7Ô∏è‚É£ Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  2  8]]\n",
      "\n",
      "8Ô∏è‚É£ Demo - Neue Vorhersage:\n",
      "Eingabe: [5.1, 3.5, 1.4, 0.2]\n",
      "Vorhersage: setosa\n",
      "Wahrscheinlichkeiten:\n",
      "  - setosa: 100.00%\n",
      "  - versicolor: 0.00%\n",
      "  - virginica: 0.00%\n",
      "\n",
      "‚úÖ Das sind die ML-Grundlagen aus dem AMALEA-Kurs!\n",
      "üöÄ Jetzt bauen wir daraus eine interaktive Streamlit-App...\n"
     ]
    }
   ],
   "source": [
    "# Installiere ML-Bibliotheken\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ ML-Pakete erfolgreich installiert!\")\n",
    "\n",
    "# üéØ ML-Grundlagen aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "# Basiert auf \"Maschinelles Lernen und seine Anwendungen\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ü§ñ Machine Learning Grundlagen - AMALEA modernisiert\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1Ô∏è‚É£ Iris-Datensatz laden (AMALEA-Klassiker)\n",
    "print(\"1Ô∏è‚É£ Iris-Datensatz laden (bekannt aus dem urspr√ºnglichen AMALEA-Kurs):\")\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "print(f\"üìä Datensatz-Info:\")\n",
    "print(f\"- Samples (Zeilen): {df.shape[0]}\")\n",
    "print(f\"- Features (Spalten): {df.shape[1]-1}\")  # -1 f√ºr target\n",
    "print(f\"- Klassen: {list(iris.target_names)}\")\n",
    "print(f\"\\nüìã Erste 5 Zeilen:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2Ô∏è‚É£ Features vs. Target (AMALEA-Konzept)\n",
    "print(f\"\\n2Ô∏è‚É£ Features vs. Target identifizieren (AMALEA-Konzept):\")\n",
    "X = iris.data  # Features (Input)\n",
    "y = iris.target  # Target (Output)\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Features (Input): {feature_names}\")\n",
    "print(f\"Target (Output): Species {target_names}\")\n",
    "\n",
    "# 3Ô∏è‚É£ Datenaufteilung (AMALEA-Standard)\n",
    "print(f\"\\n3Ô∏è‚É£ Datenaufteilung nach AMALEA-Standard:\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training: {X_train.shape[0]} Samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} Samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "# 4Ô∏è‚É£ ML-Algorithmus (Random Forest - war im urspr√ºnglichen AMALEA erw√§hnt)\n",
    "print(f\"\\n4Ô∏è‚É£ Random Forest Algorithmus (aus AMALEA-Kurs):\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5Ô∏è‚É£ Vorhersagen treffen\n",
    "print(f\"\\n5Ô∏è‚É£ Vorhersagen auf Testdaten:\")\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Genauigkeit: {accuracy:.2%}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Feature Importance (wichtig f√ºr Interpretierbarkeit)\n",
    "print(f\"\\n6Ô∏è‚É£ Feature Importance (Welche Merkmale sind wichtig?):\")\n",
    "importance = model.feature_importances_\n",
    "for name, imp in zip(feature_names, importance):\n",
    "    print(f\"- {name}: {imp:.3f}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Verwirrungsmatrix verstehen (wichtig f√ºr Evaluation)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\n7Ô∏è‚É£ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# 8Ô∏è‚É£ Demo: Neue Vorhersage\n",
    "print(f\"\\n8Ô∏è‚É£ Demo - Neue Vorhersage:\")\n",
    "new_flower = [[5.1, 3.5, 1.4, 0.2]]  # Beispiel-Werte\n",
    "prediction = model.predict(new_flower)\n",
    "probability = model.predict_proba(new_flower)\n",
    "\n",
    "print(f\"Eingabe: {new_flower[0]}\")\n",
    "print(f\"Vorhersage: {target_names[prediction[0]]}\")\n",
    "print(f\"Wahrscheinlichkeiten:\")\n",
    "for name, prob in zip(target_names, probability[0]):\n",
    "    print(f\"  - {name}: {prob:.2%}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Das sind die ML-Grundlagen aus dem AMALEA-Kurs!\")\n",
    "print(f\"üöÄ Jetzt bauen wir daraus eine interaktive Streamlit-App...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046e3d6",
   "metadata": {},
   "source": [
    "## üéØ Demo 2.1: Iris Klassifikation mit ML\n",
    "\n",
    "Wir erweitern unsere Iris-App um ML-Funktionalit√§t:\n",
    "\n",
    "### Was machen wir?\n",
    "1. **Daten laden** (Iris-Dataset)\n",
    "2. **Modell trainieren** (Random Forest)\n",
    "3. **Benutzer-Eingaben** f√ºr neue Vorhersagen\n",
    "4. **Performance anzeigen** (Genauigkeit, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f06b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_ml_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_ml_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from typing import Tuple\n",
    "\n",
    "st.set_page_config(page_title=\"Iris ML Predictor\", page_icon=\"üå∏\")\n",
    "\n",
    "st.title(\"üå∏ü§ñ Iris ML Vorhersage-App\")\n",
    "st.write(\"Trainiere ein ML-Modell und mache Vorhersagen!\")\n",
    "\n",
    "# === DATEN LADEN UND VORBEREITEN ===\n",
    "@st.cache_data\n",
    "def load_and_prepare_data() -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:\n",
    "    \"\"\"L√§dt Iris-Daten und bereitet sie f√ºr ML vor\"\"\"\n",
    "    iris = sns.load_dataset('iris')\n",
    "    \n",
    "    # Features (X) und Target (y) trennen\n",
    "    X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "    y = iris['species']\n",
    "    \n",
    "    return X, y, iris\n",
    "\n",
    "X, y, iris_data = load_and_prepare_data()\n",
    "\n",
    "# === SIDEBAR F√úR MODELL-EINSTELLUNGEN ===\n",
    "st.sidebar.header(\"üîß Modell-Einstellungen\")\n",
    "test_size = st.sidebar.slider(\"Test-Datenanteil\", 0.1, 0.5, 0.2)\n",
    "random_state = st.sidebar.number_input(\"Random State\", 0, 100, 42)\n",
    "\n",
    "# === DATEN AUFTEILEN ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "# === MODELL TRAINIEREN ===\n",
    "@st.cache_data\n",
    "def train_model(test_size: float, random_state: int):\n",
    "    \"\"\"Trainiert das ML-Modell mit gegebenen Parametern\"\"\"\n",
    "    X_train_cached, X_test_cached, y_train_cached, y_test_cached = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Random Forest Classifier erstellen und trainieren\n",
    "    model = RandomForestClassifier(random_state=random_state, n_estimators=100)\n",
    "    model.fit(X_train_cached, y_train_cached)\n",
    "    \n",
    "    # Performance berechnen\n",
    "    train_accuracy = accuracy_score(y_train_cached, model.predict(X_train_cached))\n",
    "    test_accuracy = accuracy_score(y_test_cached, model.predict(X_test_cached))\n",
    "    \n",
    "    return model, train_accuracy, test_accuracy\n",
    "\n",
    "model, train_acc, test_acc = train_model(test_size, random_state)\n",
    "\n",
    "# === HAUPTBEREICH IN TABS ===\n",
    "tab1, tab2, tab3 = st.tabs([\"üéØ Vorhersage\", \"üìä Modell-Performance\", \"üìã Daten-Explorer\"])\n",
    "\n",
    "# === TAB 1: VORHERSAGE ===\n",
    "with tab1:\n",
    "    st.header(\"üéØ Mache eine Vorhersage\")\n",
    "    st.write(\"Gib die Merkmale einer Iris-Blume ein und lass das Modell die Art vorhersagen:\")\n",
    "    \n",
    "    # Eingabe-Widgets f√ºr Features\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        sepal_length = st.number_input(\n",
    "            \"Kelchblatt L√§nge (cm)\", \n",
    "            min_value=4.0, max_value=8.0, value=5.8, step=0.1\n",
    "        )\n",
    "        sepal_width = st.number_input(\n",
    "            \"Kelchblatt Breite (cm)\", \n",
    "            min_value=2.0, max_value=4.5, value=3.0, step=0.1\n",
    "        )\n",
    "    \n",
    "    with col2:\n",
    "        petal_length = st.number_input(\n",
    "            \"Bl√ºtenblatt L√§nge (cm)\", \n",
    "            min_value=1.0, max_value=7.0, value=4.3, step=0.1\n",
    "        )\n",
    "        petal_width = st.number_input(\n",
    "            \"Bl√ºtenblatt Breite (cm)\", \n",
    "            min_value=0.1, max_value=2.5, value=1.3, step=0.1\n",
    "        )\n",
    "    \n",
    "    # Vorhersage machen\n",
    "    user_input = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "    prediction = model.predict(user_input)[0]\n",
    "    prediction_proba = model.predict_proba(user_input)[0]\n",
    "    \n",
    "    # Ergebnis anzeigen\n",
    "    st.subheader(\"üîÆ Vorhersage-Ergebnis:\")\n",
    "    st.success(f\"Die Iris-Art ist wahrscheinlich: **{prediction}**\")\n",
    "    \n",
    "    # Wahrscheinlichkeiten visualisieren\n",
    "    prob_df = pd.DataFrame({\n",
    "        'Art': model.classes_,\n",
    "        'Wahrscheinlichkeit': prediction_proba\n",
    "    })\n",
    "    \n",
    "    fig = px.bar(prob_df, x='Art', y='Wahrscheinlichkeit', \n",
    "                title=\"Vorhersage-Wahrscheinlichkeiten\",\n",
    "                color='Wahrscheinlichkeit',\n",
    "                color_continuous_scale='viridis')\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Interpretation\n",
    "    confidence = max(prediction_proba)\n",
    "    if confidence > 0.8:\n",
    "        st.success(f\"üéØ Sehr sicher! Confidence: {confidence:.1%}\")\n",
    "    elif confidence > 0.6:\n",
    "        st.warning(f\"‚ö†Ô∏è M√§√üig sicher. Confidence: {confidence:.1%}\")\n",
    "    else:\n",
    "        st.error(f\"‚ùå Unsicher. Confidence: {confidence:.1%}\")\n",
    "\n",
    "# === TAB 2: MODELL-PERFORMANCE ===\n",
    "with tab2:\n",
    "    st.header(\"üìä Modell-Performance\")\n",
    "    \n",
    "    # Key Metrics\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    with col1:\n",
    "        st.metric(\"Training Genauigkeit\", f\"{train_acc:.1%}\")\n",
    "    with col2:\n",
    "        st.metric(\"Test Genauigkeit\", f\"{test_acc:.1%}\")\n",
    "    with col3:\n",
    "        overfitting = train_acc - test_acc\n",
    "        st.metric(\"Overfitting\", f\"{overfitting:.1%}\")\n",
    "    \n",
    "    # Overfitting-Warnung\n",
    "    if overfitting > 0.1:\n",
    "        st.warning(\"‚ö†Ô∏è M√∂gliches Overfitting! Modell k√∂nnte auf neuen Daten schlechter sein.\")\n",
    "    else:\n",
    "        st.success(\"‚úÖ Gute Generalisierung!\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    st.subheader(\"üéØ Feature Wichtigkeit\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': ['Kelchblatt L√§nge', 'Kelchblatt Breite', 'Bl√ºtenblatt L√§nge', 'Bl√ºtenblatt Breite'],\n",
    "        'Wichtigkeit': model.feature_importances_\n",
    "    }).sort_values('Wichtigkeit', ascending=True)\n",
    "    \n",
    "    fig_importance = px.bar(importance_df, x='Wichtigkeit', y='Feature', \n",
    "                           orientation='h', title=\"Welche Features sind am wichtigsten?\",\n",
    "                           color='Wichtigkeit', color_continuous_scale='blues')\n",
    "    st.plotly_chart(fig_importance, use_container_width=True)\n",
    "    \n",
    "    # Erkl√§rung\n",
    "    most_important = importance_df.iloc[-1]['Feature']\n",
    "    st.write(f\"üí° **{most_important}** ist das wichtigste Merkmal f√ºr die Klassifikation!\")\n",
    "    \n",
    "    # Confusion Matrix (vereinfacht)\n",
    "    y_pred = model.predict(X_test)\n",
    "    st.subheader(\"üîç Detaillierte Performance\")\n",
    "    \n",
    "    # Classification Report als DataFrame\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    st.dataframe(report_df.round(3))\n",
    "\n",
    "# === TAB 3: DATEN-EXPLORER ===\n",
    "with tab3:\n",
    "    st.header(\"üìã Daten-Explorer\")\n",
    "    \n",
    "    # Trainings- vs Test-Daten\n",
    "    st.subheader(\"üìä Datenaufteilung\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.metric(\"Trainings-Samples\", len(X_train))\n",
    "    with col2:\n",
    "        st.metric(\"Test-Samples\", len(X_test))\n",
    "    \n",
    "    # Originaledaten anzeigen\n",
    "    st.subheader(\"üîç Original-Dataset\")\n",
    "    st.dataframe(iris_data)\n",
    "    \n",
    "    # Korrelations-Heatmap\n",
    "    st.subheader(\"üîó Feature-Korrelationen\")\n",
    "    correlation_matrix = X.corr()\n",
    "    fig_corr = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\",\n",
    "                        title=\"Wie h√§ngen die Features zusammen?\",\n",
    "                        color_continuous_scale='RdBu')\n",
    "    st.plotly_chart(fig_corr, use_container_width=True)\n",
    "    \n",
    "    # Scatter Plot Matrix\n",
    "    st.subheader(\"üìà Feature-Beziehungen\")\n",
    "    fig_scatter = px.scatter_matrix(iris_data, \n",
    "                                   dimensions=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n",
    "                                   color='species')\n",
    "    st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "\n",
    "# === FOOTER ===\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.write(\"üí° **ML-Tipp:** Je mehr gute Daten, desto besser das Modell!\")\n",
    "st.sidebar.write(\"üéØ **N√§chster Schritt:** Probiere verschiedene Modell-Parameter aus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8890f",
   "metadata": {},
   "source": [
    "## üí° ML-Konzepte erkl√§rt\n",
    "\n",
    "### Train-Test Split\n",
    "**Warum teilen wir Daten auf?**\n",
    "- **Training:** Modell lernt aus diesen Daten\n",
    "- **Test:** Pr√ºfen wie gut es bei unbekannten Daten ist\n",
    "- **80/20 oder 70/30** sind typische Aufteilungen\n",
    "\n",
    "### Random Forest\n",
    "**Was ist das?**\n",
    "- **Ensemble-Methode:** Kombiniert viele Entscheidungsb√§ume\n",
    "- **Robust:** Funktioniert gut bei vielen Problemen\n",
    "- **Feature Importance:** Zeigt, welche Variablen wichtig sind\n",
    "\n",
    "### Overfitting\n",
    "**Problem:** Modell lernt Trainingsdaten auswendig, kann aber nicht generalisieren\n",
    "**Erkennung:** Training-Performance >> Test-Performance\n",
    "**L√∂sung:** Mehr Daten, einfachere Modelle, Regularisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e7c66",
   "metadata": {},
   "source": [
    "## üéØ Aufgabe 2.1: Regression-App erstellen\n",
    "\n",
    "Jetzt erstellen wir eine App f√ºr ein **Regressions-Problem** (Zahlenwerte vorhersagen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b070cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting housing_regression_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile housing_regression_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import plotly.express as px\n",
    "from typing import Tuple, Any\n",
    "\n",
    "st.set_page_config(page_title=\"Housing Price Predictor\", page_icon=\"üè†\")\n",
    "\n",
    "st.title(\"üè†üí∞ Immobilienpreis-Vorhersage\")\n",
    "st.write(\"Sch√§tze Hauspreise basierend auf verschiedenen Merkmalen\")\n",
    "\n",
    "# === SIMULIERTE HOUSING-DATEN ERSTELLEN ===\n",
    "@st.cache_data\n",
    "def create_housing_data() -> pd.DataFrame:\n",
    "    \"\"\"Erstellt realistische Beispiel-Immobiliendaten\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # Features erstellen\n",
    "    rooms = np.random.normal(6, 1.5, n_samples)\n",
    "    rooms = np.clip(rooms, 3, 10)  # Zwischen 3 und 10 Zimmer\n",
    "    \n",
    "    age = np.random.uniform(1, 100, n_samples)\n",
    "    distance_to_city = np.random.uniform(1, 12, n_samples)\n",
    "    crime_rate = np.random.exponential(3, n_samples)\n",
    "    crime_rate = np.clip(crime_rate, 0, 15)  # Max 15\n",
    "    \n",
    "    # Target erstellen (mit realistischen Zusammenh√§ngen)\n",
    "    price = (\n",
    "        rooms * 50000 +                    # Mehr Zimmer = teurer\n",
    "        (100 - age) * 1000 +              # Neuer = teurer\n",
    "        (12 - distance_to_city) * 5000 +  # N√§her zur Stadt = teurer\n",
    "        (-crime_rate * 2000) +            # Mehr Kriminalit√§t = billiger\n",
    "        np.random.normal(0, 20000, n_samples) +  # Zuf√§lliges Rauschen\n",
    "        200000                            # Basis-Preis\n",
    "    )\n",
    "    \n",
    "    # Negative Preise vermeiden\n",
    "    price = np.maximum(price, 50000)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'rooms': rooms,\n",
    "        'age': age,\n",
    "        'distance_to_city': distance_to_city,\n",
    "        'crime_rate': crime_rate,\n",
    "        'price': price\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# === DATEN LADEN ===\n",
    "housing_data = create_housing_data()\n",
    "\n",
    "# Features und Target trennen\n",
    "feature_columns = ['rooms', 'age', 'distance_to_city', 'crime_rate']\n",
    "X = housing_data[feature_columns]\n",
    "y = housing_data['price']\n",
    "\n",
    "# === SIDEBAR F√úR MODELL-AUSWAHL ===\n",
    "st.sidebar.header(\"üîß Modell-Konfiguration\")\n",
    "model_type = st.sidebar.selectbox(\n",
    "    \"W√§hle Algorithmus:\", \n",
    "    [\"Linear Regression\", \"Random Forest\"]\n",
    ")\n",
    "test_size = st.sidebar.slider(\"Test-Datenanteil\", 0.1, 0.5, 0.2)\n",
    "\n",
    "# === MODELL TRAINIEREN ===\n",
    "@st.cache_data\n",
    "def train_regression_model(model_type: str, test_size: float) -> Tuple[Any, float, float, float, float, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Trainiert Regressions-Modell\"\"\"\n",
    "    X_train_cached, X_test_cached, y_train_cached, y_test_cached = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    if model_type == \"Linear Regression\":\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "    \n",
    "    model.fit(X_train_cached, y_train_cached)\n",
    "    \n",
    "    # Vorhersagen\n",
    "    train_pred = model.predict(X_train_cached)\n",
    "    test_pred = model.predict(X_test_cached)\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    train_r2 = r2_score(y_train_cached, train_pred)\n",
    "    test_r2 = r2_score(y_test_cached, test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_cached, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_cached, test_pred))\n",
    "    \n",
    "    return model, train_r2, test_r2, train_rmse, test_rmse, X_test_cached, y_test_cached\n",
    "\n",
    "model, train_r2, test_r2, train_rmse, test_rmse, X_test, y_test = train_regression_model(model_type, test_size)\n",
    "\n",
    "# === APP-LAYOUT ===\n",
    "tab1, tab2, tab3 = st.tabs([\"üè† Preis-Vorhersage\", \"üìà Modell-Analyse\", \"üìä Daten-√úbersicht\"])\n",
    "\n",
    "# === TAB 1: PREIS-VORHERSAGE ===\n",
    "with tab1:\n",
    "    st.header(\"üè† Immobilienpreis sch√§tzen\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        rooms = st.slider(\"Anzahl Zimmer\", 3.0, 10.0, 6.0, 0.1)\n",
    "        age = st.slider(\"Alter des Hauses (Jahre)\", 1, 100, 30)\n",
    "    \n",
    "    with col2:\n",
    "        distance = st.slider(\"Entfernung zur Stadt (km)\", 1.0, 12.0, 5.0, 0.1)\n",
    "        crime_rate = st.slider(\"Kriminalit√§tsrate\", 0.0, 15.0, 3.0, 0.1)\n",
    "    \n",
    "    # Vorhersage\n",
    "    user_input = np.array([[rooms, age, distance, crime_rate]])\n",
    "    predicted_price = model.predict(user_input)[0]\n",
    "    \n",
    "    st.subheader(\"üí∞ Gesch√§tzter Preis:\")\n",
    "    st.success(f\"**${predicted_price:,.0f}**\")\n",
    "    \n",
    "    # Preis-Kategorisierung\n",
    "    if predicted_price < 300000:\n",
    "        st.info(\"üè† G√ºnstiges Segment\")\n",
    "    elif predicted_price < 600000:\n",
    "        st.warning(\"üèòÔ∏è Mittleres Segment\")\n",
    "    else:\n",
    "        st.error(\"üè∞ Luxus-Segment\")\n",
    "    \n",
    "    # Vergleich mit √§hnlichen H√§usern\n",
    "    st.subheader(\"üìä Vergleich mit √§hnlichen H√§usern\")\n",
    "    similar_houses = housing_data[\n",
    "        (abs(housing_data['rooms'] - rooms) < 1) &\n",
    "        (abs(housing_data['age'] - age) < 10)\n",
    "    ]\n",
    "    \n",
    "    if len(similar_houses) > 0:\n",
    "        avg_similar_price = similar_houses['price'].mean()\n",
    "        difference = predicted_price - avg_similar_price\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.metric(\"Durchschnitt √§hnlicher H√§user\", f\"${avg_similar_price:,.0f}\")\n",
    "        with col2:\n",
    "            st.metric(\"Unterschied\", f\"${difference:,.0f}\")\n",
    "\n",
    "# === TAB 2: MODELL-ANALYSE ===\n",
    "with tab2:\n",
    "    st.header(\"üìà Modell-Performance\")\n",
    "    \n",
    "    # Performance Metriken\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    with col1:\n",
    "        st.metric(\"Train R¬≤\", f\"{train_r2:.3f}\")\n",
    "    with col2:\n",
    "        st.metric(\"Test R¬≤\", f\"{test_r2:.3f}\")\n",
    "    with col3:\n",
    "        st.metric(\"Train RMSE\", f\"${train_rmse:,.0f}\")\n",
    "    with col4:\n",
    "        st.metric(\"Test RMSE\", f\"${test_rmse:,.0f}\")\n",
    "    \n",
    "    # R¬≤ Erkl√§rung\n",
    "    st.write(\"**R¬≤ (R-Squared) Interpretation:**\")\n",
    "    if test_r2 > 0.8:\n",
    "        st.success(\"üéØ Sehr gutes Modell!\")\n",
    "    elif test_r2 > 0.6:\n",
    "        st.warning(\"‚ö†Ô∏è OK-es Modell\")\n",
    "    else:\n",
    "        st.error(\"‚ùå Schwaches Modell\")\n",
    "    \n",
    "    st.write(f\"Das Modell erkl√§rt {test_r2:.1%} der Preisvarianz.\")\n",
    "    \n",
    "    # Vorhersage vs. Realit√§t Plot\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Echte Preise': y_test, \n",
    "        'Vorhergesagte Preise': y_pred\n",
    "    })\n",
    "    \n",
    "    fig_pred = px.scatter(\n",
    "        pred_df, \n",
    "        x='Echte Preise', \n",
    "        y='Vorhergesagte Preise',\n",
    "        title=\"Vorhersage vs. Realit√§t\"\n",
    "    )\n",
    "    # Perfekte Linie hinzuf√ºgen\n",
    "    min_price = min(pred_df['Echte Preise'].min(), pred_df['Vorhergesagte Preise'].min())\n",
    "    max_price = max(pred_df['Echte Preise'].max(), pred_df['Vorhergesagte Preise'].max())\n",
    "    fig_pred.add_shape(\n",
    "        type=\"line\", \n",
    "        x0=min_price, y0=min_price,\n",
    "        x1=max_price, y1=max_price,\n",
    "        line=dict(color=\"red\", dash=\"dash\")\n",
    "    )\n",
    "    st.plotly_chart(fig_pred, use_container_width=True)\n",
    "    \n",
    "    # Feature Importance (nur f√ºr Random Forest)\n",
    "    if model_type == \"Random Forest\":\n",
    "        st.subheader(\"üéØ Feature Wichtigkeit\")\n",
    "        feature_names = ['Zimmer', 'Alter', 'Entfernung zur Stadt', 'Kriminalit√§tsrate']\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Wichtigkeit': model.feature_importances_\n",
    "        }).sort_values('Wichtigkeit', ascending=True)\n",
    "        \n",
    "        fig_imp = px.bar(\n",
    "            importance_df, \n",
    "            x='Wichtigkeit', \n",
    "            y='Feature',\n",
    "            orientation='h',\n",
    "            title=\"Welche Faktoren beeinflussen den Preis am meisten?\"\n",
    "        )\n",
    "        st.plotly_chart(fig_imp, use_container_width=True)\n",
    "\n",
    "# === TAB 3: DATEN-√úBERSICHT ===\n",
    "with tab3:\n",
    "    st.header(\"üìä Datensatz-√úbersicht\")\n",
    "    \n",
    "    # Basis-Statistiken\n",
    "    st.subheader(\"üìã Grundlegende Statistiken\")\n",
    "    st.dataframe(housing_data.describe(), use_container_width=True)\n",
    "    \n",
    "    # Rohdaten\n",
    "    st.subheader(\"üîç Rohdaten (Erste 20 Zeilen)\")\n",
    "    st.dataframe(housing_data.head(20))\n",
    "    \n",
    "    # Verteilungen der Features\n",
    "    st.subheader(\"üìà Feature-Verteilungen\")\n",
    "    feature_to_plot = st.selectbox(\n",
    "        \"Feature f√ºr Histogramm:\", \n",
    "        ['rooms', 'age', 'distance_to_city', 'crime_rate', 'price']\n",
    "    )\n",
    "    \n",
    "    fig_hist = px.histogram(\n",
    "        housing_data, \n",
    "        x=feature_to_plot, \n",
    "        title=f\"Verteilung von {feature_to_plot}\",\n",
    "        nbins=30\n",
    "    )\n",
    "    st.plotly_chart(fig_hist, use_container_width=True)\n",
    "    \n",
    "    # Korrelations-Matrix\n",
    "    st.subheader(\"üîó Korrelationen\")\n",
    "    corr_matrix = housing_data.corr()\n",
    "    fig_corr = px.imshow(\n",
    "        corr_matrix,\n",
    "        text_auto=True,\n",
    "        aspect=\"auto\",\n",
    "        title=\"Korrelations-Matrix\"\n",
    "    )\n",
    "    st.plotly_chart(fig_corr, use_container_width=True)\n",
    "\n",
    "# === FOOTER ===\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.write(\"üí° **Regression-Tipp:** R¬≤ n√§her bei 1 = besseres Modell\")\n",
    "st.sidebar.write(\"üéØ **Ausprobieren:** Ver√§ndere die Eingaben und beobachte die Auswirkungen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb514866",
   "metadata": {},
   "source": [
    "## üèÜ Lernziel-Check Woche 2\n",
    "\n",
    "Am Ende dieser Woche solltest du:\n",
    "\n",
    "- [ ] **ML-Grundlagen verstehen:** Klassifikation vs. Regression, Training vs. Test\n",
    "- [ ] **ML-Modelle in Streamlit integrieren:** Scikit-learn + Streamlit\n",
    "- [ ] **Benutzer-Eingaben f√ºr Vorhersagen nutzen:** Input-Widgets ‚Üí Predictions\n",
    "- [ ] **Modell-Performance bewerten:** Accuracy, R¬≤, Overfitting erkennen\n",
    "- [ ] **Feature Importance interpretieren:** Welche Variablen sind wichtig?\n",
    "- [ ] **Eine funktionsf√§hige ML-App entwickelt haben:** Vollst√§ndig und benutzerfreundlich\n",
    "\n",
    "## üìù Vorbereitung f√ºr die Fallstudie\n",
    "\n",
    "**Denke bereits jetzt √ºber dein Fallstudie-Projekt nach:**\n",
    "\n",
    "### Projektideen f√ºr deine Fallstudie:\n",
    "1. **Kunden-Segmentierung** (E-Commerce, Bank)\n",
    "2. **Preis-Vorhersage** (Immobilien, Autos, Aktien)\n",
    "3. **Klassifikation** (Spam-Erkennung, Sentiment-Analyse)\n",
    "4. **Gesundheitsdaten** (Krankheits-Vorhersage, Fitness-Tracking)\n",
    "5. **Sport-Analytics** (Spieler-Performance, Ergebnis-Vorhersagen)\n",
    "\n",
    "### Fragen f√ºr dein Projekt:\n",
    "1. **Welches Problem m√∂chtest du l√∂sen?**\n",
    "2. **Welche Daten brauchst du?** (CSV, API, Web-Scraping?)\n",
    "3. **Wer ist deine Zielgruppe?** (Manager, Endkunden, Experten?)\n",
    "4. **Was ist das gew√ºnschte Ergebnis?** (Klassifikation, Regression, Clustering?)\n",
    "\n",
    "## üîÆ Ausblick auf Woche 3\n",
    "\n",
    "N√§chste Woche werden wir:\n",
    "- **Apps online deployen** (Streamlit Cloud, Heroku)\n",
    "- **Externe Datenquellen** einbinden (APIs, Datenbanken)\n",
    "- **App-Performance optimieren** (Caching, Lazy Loading)\n",
    "- **Professional Design** (Styling, UX/UI Verbesserungen)\n",
    "\n",
    "**üí° Tipp:** Beginne bereits jetzt mit der Sammlung von Daten f√ºr deine Fallstudie!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
