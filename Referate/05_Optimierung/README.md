# Gradient Descent Update

Formel:

$$w_{t+1} = w_t - \eta \nabla_w \mathcal{L}$$

Kurz: Basis-Updateregel f√ºr iterative Optimierung (Lernrate $\eta$).