name: Performance

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run performance tests weekly
    - cron: '0 2 * * 1'
  workflow_dispatch:
    inputs:
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '60'
        type: string
      users:
        description: 'Number of concurrent users'
        required: false
        default: '10'
        type: string

jobs:
  # Unit performance tests
  unit-performance:
    name: Unit Performance Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r mc_test_app/requirements.txt
          pip install pytest-benchmark pytest-codspeed

      - name: Run performance benchmarks
        run: |
          pytest mc_test_app/tests/ \
            --benchmark-only \
            --benchmark-json=benchmark.json \
            --benchmark-compare \
            --benchmark-compare-fail=min:5% \
            --benchmark-histogram=benchmark-histogram.json \
            -v

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: unit-benchmarks
          path: |
            benchmark.json
            benchmark-histogram.json

      - name: Comment PR with benchmark results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const benchmark = JSON.parse(fs.readFileSync('benchmark.json', 'utf8'));

            let comment = '## ðŸƒ Performance Benchmark Results\n\n';
            comment += '| Test | Time | Change |\n';
            comment += '|------|------|--------|\n';

            benchmark.benchmarks.forEach(bench => {
              const time = bench.stats.mean.toFixed(4);
              const change = bench.stats.rounds > 1 ? 'Â±0%' : 'N/A';
              comment += `| ${bench.name} | ${time}s | ${change} |\n`;
            });

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Load testing
  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'schedule'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r mc_test_app/requirements.txt
          pip install locust

      - name: Create Locust test file
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between

          class MCAppUser(HttpUser):
              wait_time = between(1, 3)

              @task
              def load_main_page(self):
                  self.client.get("/")

              @task(3)
              def simulate_quiz_interaction(self):
                  # Simulate quiz interaction
                  self.client.post("/_stcore/stream", json={
                      "question": "test",
                      "answer": "test_answer"
                  })

              @task
              def check_health(self):
                  self.client.get("/health")
          EOF

      - name: Start application
        run: |
          streamlit run mc_test_app/mc_test_app.py --server.headless true --server.port 8501 &
          sleep 30

      - name: Run load test
        run: |
          locust \
            --headless \
            --users ${{ github.event.inputs.users || 10 }} \
            --spawn-rate 2 \
            --run-time ${{ github.event.inputs.duration || 60 }}s \
            --host http://localhost:8501 \
            --csv=load_test_results

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            load_test_results_*.csv

  # Memory and resource monitoring
  resource-monitoring:
    name: Resource Monitoring
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r mc_test_app/requirements.txt
          pip install memory-profiler psutil

      - name: Run memory profiling
        run: |
          python -c "
          import mc_test_app.core as core
          from memory_profiler import profile

          @profile
          def test_memory_usage():
              # Test core functions for memory usage
              user_id = 'test_user'
              hashed = core.get_user_id_hash(user_id)
              print(f'User hash: {hashed}')

              # Test session state operations
              import streamlit as st
              # Mock session state for testing
              print('Memory profiling completed')

          test_memory_usage()
          " > memory_profile.txt

      - name: Upload memory profile
        uses: actions/upload-artifact@v4
        with:
          name: memory-profile
          path: memory_profile.txt

  # Lighthouse performance audit
  lighthouse:
    name: Lighthouse Performance Audit
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'schedule'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install Lighthouse
        run: npm install -g lighthouse

      - name: Start application
        run: |
          streamlit run mc_test_app/mc_test_app.py --server.headless true --server.port 8501 &
          sleep 30

      - name: Run Lighthouse audit
        run: |
          lighthouse http://localhost:8501 \
            --output json \
            --output html \
            --output-path ./lighthouse-report.json \
            --chrome-flags="--headless --no-sandbox --disable-gpu"

      - name: Upload Lighthouse report
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-report
          path: |
            lighthouse-report.json
            lighthouse-report.html

      - name: Comment PR with Lighthouse results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('lighthouse-report.json', 'utf8'));

            const performance = Math.round(report.categories.performance.score * 100);
            const accessibility = Math.round(report.categories.accessibility.score * 100);
            const seo = Math.round(report.categories.seo.score * 100);

            const comment = `## ðŸ“Š Lighthouse Performance Audit

            | Metric | Score |
            |--------|-------|
            | Performance | ${performance}/100 |
            | Accessibility | ${accessibility}/100 |
            | SEO | ${seo}/100 |

            [View full report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Database performance (if applicable)
  database-performance:
    name: Database Performance
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r mc_test_app/requirements.txt

      - name: Run database performance tests
        run: |
          python -c "
          import mc_test_app.core as core
          import time
          import pandas as pd

          # Test CSV operations performance
          start_time = time.time()
          for i in range(100):
              # Simulate saving answers
              pass
          end_time = time.time()

          print(f'CSV operations took: {end_time - start_time:.4f} seconds')
          print('Database performance test completed')
          " > db_performance.txt

      - name: Upload database performance results
        uses: actions/upload-artifact@v4
        with:
          name: db-performance
          path: db_performance.txt