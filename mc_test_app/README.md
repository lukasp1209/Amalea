# ğŸ“ MC-Test Streamlit App

[![CI](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml)

Eine interaktive Multiple-Choice-Lern- und Selbsttest-App fÃ¼r Kursteilnehmer/innen.
Bietet schnelles Feedback, Fortschrittsverfolgung und aggregierte Ergebnisse
fÃ¼r diverse Fragensets.

---

## ğŸš€ Ãœbersicht

Diese App ist ein vollstÃ¤ndiger MC-Test fÃ¼r Kursinhalte, entwickelt mit Streamlit.
Sie ermÃ¶glicht anonyme Tests mit Pseudonymen, zufÃ¤lliger Fragenreihenfolge und Zeitlimit.
Perfekt fÃ¼r Bildungsumgebungen oder Selbstlernphasen.


### Hauptfunktionen (Stand 2025-09-24, verifiziert)

| Kategorie      | Funktion (verifiziert)                                                                                 |
|---------------|--------------------------------------------------------------------------------------------------------|
| Zugang        | Pseudonym-Login (anonymisiert via Hash)                                                                |
| Fragen        | ZufÃ¤llige Reihenfolge, Gewichtung je Frage, ErklÃ¤rungen, **strikte Trennung nach Fragenset**           |
| Fragenset     | Auswahl & Persistenz des Fragensets (Fragenpool) auf Startseite, Query-Param-Sync, keine Vermischung   |
| Scoring-Modi  | "Nur +Punkte" (falsch = 0) Â· "+/- Punkte" (falsch = -Gewichtung, ab 2025-09-22 volle Gewichtung)     |
| Feedback      | Sofortiges Ergebnis + ErklÃ¤rung, dynamische Motivation                                                 |
| Fortschritt   | Persistenz pro Pseudonym (Session lokal, pro Fragenset getrennt)                                       |
| Zeitlimit     | Optionales 60-Minuten-Fenster (abschaltbar durch Code-Anpassung)                                      |
| Leaderboard   | Ã–ffentliches Topâ€‘5 vor Login; vollstÃ¤ndige Ansicht fÃ¼r Admin                                           |
| Analyse       | Itemanalyse (p, r_pb, Distraktor, Verteilungen)                                                        |
| Export        | CSV-Download Ã¼ber Admin-Panel, **normiertes Schema**                                                   |
| Reset         | Globaler CSV-Reset mit Hinweisbanner & BestÃ¤tigungsdialog (System-Tab, Admin)                         |
| Admin-Panel   | Sichtbar & funktionsfÃ¤hig nach Login, Session-Handling, keine doppelten Widget-Keys                    |
| Sicherheit    | Hashing + Admin-Key + Rate-Limit (optional), DEV-Fallback                                              |
| Accessibility | Reduzierte Animationen, hoher Kontrast                                                                |

**Neu (2025-09-22 bis 2025-09-24, verifiziert):**

- Strikte Trennung & Persistenz der Antworten, Bookmarks und Exporte pro Fragenset (kein Pool-Mix mehr mÃ¶glich)
- Admin-Panel: Sichtbarkeit, Session-Handling und Reset-Button mit BestÃ¤tigung verbessert
- CSV-Export: Spaltenreihenfolge und Schema sind jetzt immer konsistent
- Fragenset-Auswahl: Persistenz via Query-Param und Session, keine Vermischung nach Wechsel
- Bugfixes: Keine doppelten Widget-Keys, keine unerwÃ¼nschten Titel im Admin-Panel, keine Frage-Mischung

Alle Features wurden am 2025-09-24 getestet und funktionieren wie dokumentiert.

---

## ğŸ‘¨â€ğŸ’» Entwickler-Info: Session State Variablen

Die App verwendet `st.session_state` intensiv zur Steuerung von UI, Fortschritt, Authentifizierung und Pool-Logik. Nachfolgend eine Ãœbersicht der wichtigsten Session-Variablen und ihrer Bedeutung (Stand 2025-09-24):

| Variable                  | Typ         | Bedeutung                                                                                 |
|---------------------------|-------------|------------------------------------------------------------------------------------------|
| user_id                   | str         | Aktuelles Pseudonym (Plaintext, fÃ¼r Leaderboard & Anzeige)                               |
| user_id_hash              | str         | SHA-256-Hash des Pseudonyms (fÃ¼r AnonymitÃ¤t, als Key fÃ¼r Antworten)                      |
| user_id_display           | str         | GekÃ¼rzter Hash (z.B. erste 10 Zeichen, fÃ¼r Leaderboard)                                  |
| selected_questions_file   | str         | Aktuell gewÃ¤hltes Fragenset (Dateiname, z.B. `questions_Data_Science.json`)              |
| beantwortet               | list[bool]  | Liste, ob jede Frage beantwortet wurde (Index = Frage)                                   |
| frage_indices             | list[int]   | Reihenfolge der Fragen (zufÃ¤llig permutiert)                                             |
| optionen_shuffled         | list[list]  | FÃ¼r jede Frage: zufÃ¤llig permutierte Antwortoptionen                                     |
| answers_text              | list[str]   | Vom User gewÃ¤hlte Antworttexte (Index = Frage)                                           |
| answer_outcomes           | list[int]   | Punktwert pro Frage (Index = Frage)                                                      |
| celebrated_questions      | set/int     | IDs der Fragen, fÃ¼r die bereits ein Motivationsbanner gezeigt wurde                      |
| start_zeit                | str/dt      | ISO8601-Startzeit des Tests (fÃ¼r Zeitlimit)                                              |
| test_time_expired         | bool        | True, wenn Zeitlimit Ã¼berschritten                                                       |
| bookmarks                 | set/int     | Vom User markierte Fragen (Bookmark-Feature)                                             |
| admin_auth_ok             | bool        | True, wenn Admin-Login erfolgreich                                                       |
| show_admin_panel          | bool        | True, wenn Admin-Panel angezeigt werden soll                                              |
| admin_view                | str         | Aktueller Tab im Admin-Panel (z.B. "Leaderboard", "Analyse", "System")               |
| __selected_pool_tmp       | str         | Zwischenspeicher fÃ¼r Fragenset-Auswahl (Selectbox)                                       |
| __admin_reset_confirm     | bool        | True, wenn Admin-Reset bestÃ¤tigt wurde                                                   |
| __admin_reset_pending     | bool        | True, wenn Admin-Reset-Dialog angezeigt wird                                             |
| __admin_reset_done        | bool        | True, wenn Admin-Reset durchgefÃ¼hrt wurde                                                |

Weitere temporÃ¤re oder Feature-spezifische Variablen kÃ¶nnen im Code ergÃ¤nzt werden. Die wichtigsten States werden beim Fragenset-Wechsel und beim globalen Reset gezielt gelÃ¶scht oder neu initialisiert.

**Hinweis:** Die Session-State-Keys sind bewusst sprechend gewÃ¤hlt und kÃ¶nnen sich bei neuen Features erweitern. FÃ¼r robuste Feature-Entwicklung empfiehlt sich die Nutzung von `st.session_state.get("key")` mit Defaultwerten.
# ğŸ“ MC-Test Streamlit App

[![CI](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml)

Eine interaktive Multiple-Choice-Lern- und Selbsttest-App fÃ¼r Kursteilnehmer.
Bietet schnelles Feedback, Fortschrittsverfolgung und aggregierte Ergebnisse
fÃ¼r Data Science-Themen.

---

## ğŸš€ Ãœbersicht

Diese App ist ein vollstÃ¤ndiger MC-Test fÃ¼r Data Analytics, entwickelt mit Streamlit.
Sie ermÃ¶glicht anonyme Tests mit Pseudonymen, zufÃ¤lliger Fragenreihenfolge und Zeitlimit.
Perfekt fÃ¼r Bildungsumgebungen oder Selbstlernphasen.


### Hauptfunktionen (Stand 2025-09-24, verifiziert)

| Kategorie      | Funktion (verifiziert)                                                                                 |
|---------------|--------------------------------------------------------------------------------------------------------|
| Zugang        | Pseudonym-Login (anonymisiert via Hash)                                                                |
| Fragen        | ZufÃ¤llige Reihenfolge, Gewichtung je Frage, ErklÃ¤rungen, **strikte Trennung nach Fragenset**           |
| Fragenset     | Auswahl & Persistenz des Fragensets (Fragenpool) auf Startseite, Query-Param-Sync, keine Vermischung   |
| Scoring-Modi  | "Nur +Punkte" (falsch = 0) Â· "+/- Punkte" (falsch = -Gewichtung, ab 2025-09-22 volle Gewichtung)     |
| Feedback      | Sofortiges Ergebnis + ErklÃ¤rung, dynamische Motivation                                                 |
| Fortschritt   | Persistenz pro Pseudonym (Session lokal, pro Fragenset getrennt)                                       |
| Zeitlimit     | Optionales 60-Minuten-Fenster (abschaltbar durch Code-Anpassung)                                      |
| Leaderboard   | Ã–ffentliches Topâ€‘5 vor Login; vollstÃ¤ndige Ansicht fÃ¼r Admin                                           |
| Analyse       | Itemanalyse (p, r_pb, Distraktor, Verteilungen)                                                        |
| Export        | CSV-Download Ã¼ber Admin-Panel, **normiertes Schema**                                                   |
| Reset         | Globaler CSV-Reset mit Hinweisbanner & BestÃ¤tigungsdialog (System-Tab, Admin)                         |
| Admin-Panel   | Sichtbar & funktionsfÃ¤hig nach Login, Session-Handling, keine doppelten Widget-Keys                    |
| Sicherheit    | Hashing + Admin-Key + Rate-Limit (optional), DEV-Fallback                                              |
| Accessibility | Reduzierte Animationen, hoher Kontrast                                                                |

**Neu (2025-09-22 bis 2025-09-24, verifiziert):**

- Strikte Trennung & Persistenz der Antworten, Bookmarks und Exporte pro Fragenset (kein Pool-Mix mehr mÃ¶glich)
- Admin-Panel: Sichtbarkeit, Session-Handling und Reset-Button mit BestÃ¤tigung verbessert
- CSV-Export: Spaltenreihenfolge und Schema sind jetzt immer konsistent
- Fragenset-Auswahl: Persistenz via Query-Param und Session, keine Vermischung nach Wechsel
- Bugfixes: Keine doppelten Widget-Keys, keine unerwÃ¼nschten Titel im Admin-Panel, keine Frage-Mischung

Alle Features wurden am 2025-09-24 getestet und funktionieren wie dokumentiert.

---

## ğŸ“‹ Voraussetzungen

- **Python:** Version 3.8 oder hÃ¶her.
- **AbhÃ¤ngigkeiten:** Installiere via `pip install -r requirements.txt`.
- **Optionale Tools:** Docker fÃ¼r Container-Deployment; Git fÃ¼r Versionierung.

---

## ğŸ› ï¸ Installation und Start

### Lokaler Start (Empfohlen fÃ¼r Entwicklung)

1. Klone das Repository oder navigiere zum `mc_test_app/`-Ordner.
2. Installiere AbhÃ¤ngigkeiten:

   ```bash
   pip install -r requirements.txt
   ```

3. Starte die App:

   ```bash
   streamlit run mc_test_app.py
   ```

4. Ã–ffne [http://localhost:8501](http://localhost:8501) im Browser.

### Docker-Start

```bash
docker compose up -d streamlit-slim
```

FÃ¼r den vollen Stack (mit Jupyter, MLflow):

```bash
docker compose up -d
```

### Deployment (z.B. Streamlit Cloud)

1. Pushe nur den `mc_test_app/`-Ordner in ein separates Repo.
2. Verwende `git subtree` fÃ¼r saubere Trennung:

   ```bash
   git subtree push --prefix mc_test_app github main
   ```

3. Deploye auf Streamlit Cloud oder Ã¤hnlichen Plattformen.

---

## âš™ï¸ Konfiguration

### Umgebungsvariablen (`.env`-Datei)

Erstelle eine `.env`-Datei basierend auf `.env.example`:

```env
MC_TEST_ADMIN_USER=dein_admin_pseudonym  # Optional: BeschrÃ¤nkt Admin-Zugang
MC_TEST_ADMIN_KEY=dein_geheimes_passwort  # Erforderlich fÃ¼r Admin-Features
MC_TEST_MIN_SECONDS_BETWEEN=5  # Optional: Mindestsekunden zwischen Antworten
```

- **Admin-Zugang:** Ohne `MC_TEST_ADMIN_KEY` reicht ein beliebiges Passwort;
  mit Key muss es exakt passen.
- **Rate-Limiting:** Verhindert Spam; Standard: 0 (kein Limit).

### Streamlit-Secrets (`.streamlit/secrets.toml`)

FÃ¼r Produktion:

```toml
"MC_TEST_ADMIN_USER" = "admin"
"MC_TEST_ADMIN_KEY" = "secret123"
"MC_TEST_MIN_SECONDS_BETWEEN" = 5
```

Hinweise (Streamlit Cloud / TOML Parser):

- SchlÃ¼ssel UND Werte als Strings konsequent quoten (robusteste Variante): `"KEY" = "wert"`.
- Numerische Werte (z.B. `5`) kÃ¶nnen ohne Quotes, dÃ¼rfen aber auch mit `"5"` â€“ intern wird gecastet.
- Keine `.env`-Syntax (`KEY=value` ohne Leerzeichen) in `secrets.toml` verwenden â€“ immer `KEY = VALUE` mit Leerzeichen.
- Pro Zeile genau ein Key. Keine Inline-Kommentare direkt hinter dem Wert.
- Unsichtbare Sonderzeichen vermeiden (non-breaking space, typogr. Bindestrich) â€“ bei Copy/Paste ggf. sÃ¤ubern.
- Bei "invalid TOML": Quotes, `=` AbstÃ¤nde und Tabs (verboten) prÃ¼fen.

Minimalvariante (alle Strings explizit in Quotes):

```toml
"MC_TEST_ADMIN_USER" = "Admin"
"MC_TEST_ADMIN_KEY" = "Admin"
"MC_TEST_MIN_SECONDS_BETWEEN" = 1
```

### Datenpersistenz (CSV)

- **Datei:** `mc_test_answers.csv` (automatische Erstellung).
- **Schema (seit Sept 2025, kompatibel rÃ¼ckwÃ¤rts):**

  ```csv
  user_id_hash,user_id_display,user_id_plain,frage_nr,frage,antwort,richtig,zeit,markiert,questions_file
  ```

- **Felder:**

  - `user_id_hash`: SHA-256-Hash des Pseudonyms (fÃ¼r AnonymitÃ¤t).
  - `user_id_display`: GekÃ¼rzter Hash (z.B. erste 10 Zeichen).
  - `user_id_plain`: Eingetragenes Pseudonym (fÃ¼r Leaderboard).
  - `frage_nr`: Fragenummer.
  - `frage`: VollstÃ¤ndiger Fragetext.
  - `antwort`: AusgewÃ¤hlte Option.
  - `richtig`: Punktwert der Antwort. `positive_only`: +Gewichtung oder 0. `+/-`: +Gewichtung oder -Gewichtung.
  - `zeit`: ISO8601-Zeitstempel.

- **Eigenschaften:** Append-only, Pandas-kompatibel, leicht zu sichern.

---

## ğŸ“ Projektstruktur

```
mc_test_app/
â”œâ”€â”€ README.md                 # Diese Dokumentation
â”œâ”€â”€ mc_test_app.py            # Hauptapp (UI + kombinierte Logik â€“ wird schrittweise entschlackt)
â”œâ”€â”€ core.py                   # Speicher/Hash/CSV-Basisfunktionen
â”œâ”€â”€ scoring.py                # (Neu) Zentrale Score-/Leaderboard-Berechnung (Top-5 Abbildung)
â”œâ”€â”€ questions.json            # Fragenkatalog (JSON)
â”œâ”€â”€ requirements.txt          # AbhÃ¤ngigkeiten
â”œâ”€â”€ mc_test_answers.csv       # Antwort-Logs (auto-generiert)
â”œâ”€â”€ .env / .env.example       # ENV-Konfiguration
â”œâ”€â”€ __init__.py               # Paket-Marker
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json     # Dev-Container-Konfiguration
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ mc_test_app_ci.yml # Subtree-spezifischer CI-Workflow
â”œâ”€â”€ .streamlit/
â”‚   â”œâ”€â”€ config.toml           # Streamlit-Konfiguration
â”‚   â””â”€â”€ secrets.toml          # Secrets (fÃ¼r Produktion)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_core.py          # Kern-/App-Tests (Import-fallback)
â”‚   â”œâ”€â”€ test_edge_cases.py    # Edge Cases (Duplicate Guard, Leaderboard leer usw.)
â”‚   â”œâ”€â”€ test_storage.py       # File-Locking & Parallel-Append
â”‚   â””â”€â”€ test_ui.py            # UI-Sanity via streamlit.testing
â””â”€â”€ __pycache__/              # App-Cache
```

### Modularisierungsstand (Stand 2025â€‘09â€‘21)

| Modul | Zweck | Status |
|-------|-------|--------|
| `core.py` | CSV-Persistenz, Locking, Hashing, Fragenladen | Stabil |
| `scoring.py` | Punktestand, Prozent, abstrahiertes Leaderboard | Aktiv |
| `leaderboard.py` | Aggregationen, Leaderboard, Log-Ansicht (Admin) | Aktiv |
| `review.py` | Itemanalyse (Analyse / Export / System / Glossar Tabs) | Aktiv |
| `mc_test_app.py` | UI-Orchestrierung + Wrapper | Schlank |
| `gamification.py` | Badges, Streak, Motivation | Geplant |

Backward Compatibility: Wrapper-Funktionen im Hauptmodul behalten alte Namen
(`calculate_leaderboard`, `display_admin_panel` etc.), damit bestehende Tests
& externe Automationen nicht brechen.

### Warum Auslagerung?

- Geringere KomplexitÃ¤t (maintainable, testbar, klarere Verantwortlichkeiten)
- Saubere Trennung: UI vs. Analyse-/Aggregationslogik
- Wiederverwendbarkeit (spÃ¤ter ggf. Headless-Auswertung / API / Batch Reports)
- Besseres Onboarding neuer Contributor (kleinere Module)

### Neue / Erweiterte Funktionen seit Modularisierung

| Bereich | Ã„nderung | Nutzen |
|---------|----------|-------|
| Admin Auth | USER + KEY Pflicht (konstante Zeit) | Schutz |
| DEV Fallback | Auto-Credentials bei fehlender ENV | Schnelles Testen |
| Itemanalyse | p, r_pb, QualitÃ¤tslabel | Transparenz |
| Distraktor-Analyse | Dominanter Distraktor %, hÃ¤ufigste falsche | Diagnose |
| Detail-Ansicht | Verlauf + Verteilung pro Option | Item-Diagnose |
| System-Metriken | Nutzer, aktiv <10m, Ã˜ Antworten, Accuracy | Monitoring |
| Export-Tab | CSV-Download + Spaltenliste | Weiterverarb. |
| Glossar-Tab | Definitionen + Formeln | Kontext |
| Struktur | Module + Wrapper | Klarheit |

Geplante ErgÃ¤nzung: Auslagerung von Streak/Badges/Motivationslogik nach `gamification.py`.

### Scoring & Gewichtung

| Modus | Richtig | Falsch | Motivation |
|-------|---------|--------|------------|
| Nur +Punkte | +Gewichtung | 0 | Risikoarmes Ãœben |
| +/- Punkte | +Gewichtung | -Gewichtung | FÃ¶rdert sorgfÃ¤ltiges Antworten |

Hinweise:
- Gewichtung fehlt? â†’ Standard = 1.
- Prozentanzeige = aktueller Score / Summe aller Gewichtungen.
- Negative Gesamtwerte sind erlaubt (kein Floor). Optional konfigurierbar (Code-Anpassung in `current_score`).
- Vorschau-AbzÃ¼ge (Vorwarnung) kÃ¶nnen leicht ergÃ¤nzt werden (siehe Developer Guide Roadmap).

### Admin-Panel Ãœbersicht

Zwei Ebenen der Verwaltung:

Analyse-/Review (`review.py`):
- ğŸ“Š Analyse: Itemanalyse (p, r_pb, Distraktor, Verteilung)
- ğŸ“¤ Export: CSV-Download + Spaltenliste
- ğŸ›  System: Teilnehmer, AktivitÃ¤t (<10m), Ã˜ Antworten, Accuracy
- ğŸ“š Glossar: Definitionen, Hinweise, Formeln

Leaderboard (Admin):
- ğŸ¥‡ Top 5: Abgeschlossene Teilnahmen (Top 3 mit Icons)
- ğŸ‘¥ Alle Teilnahmen: Ãœbersicht aller Nutzer
- ğŸ“„ Rohdaten: Basis-Log

Ã–ffentlich (nicht angemeldet) sichtbar: Eine kompakte Topâ€‘5 Liste (ohne Detail-Logs).

Glossar-Formeln: p, r_pb, Dominanter Distraktor %.
Hinweis: Kleine Stichproben (<20) vorsichtig interpretieren.

### Integration der modularen Funktionen

`mc_test_app.py` delegiert via Wrapper an `scoring`, `leaderboard`, `review`.
Fehlschlagende Importe (Spezialumgebung) aktivieren Fallbacks.

---

## ğŸ”’ Datenschutz & Sicherheit

- **AnonymitÃ¤t:** Pseudonyme werden gehasht; nur Admins sehen Plaintext-Pseudonyme.
- **Lokale Speicherung:** Keine externen Server; Daten bleiben auf dem GerÃ¤t.
- **Admin-Schutz:** GeschÃ¼tzt durch ENV-Variablen; kein Zugriff ohne Key.
- **Rate-Limiting:** Verhindert Missbrauch (konfigurierbar).
- **Backup:** Sichere die CSV regelmÃ¤ÃŸig (z.B. via Git oder Cron).

**Hinweis:** Bei sensiblen Daten teste in isolierter Umgebung.

---

## ğŸ› ï¸ Admin & Wartung

### Admin-Bereich

- Zugang: Sidebar > Management > Key eingeben
  (nur spezifizierter Admin-User sieht das Eingabefeld).
- Tabs: Leaderboard (Top / Alle / Rohdaten), Analyse (Itemanalyse),
  Export (CSV), System (Status/KPIs), Glossar.
- Scoring-Modus-Umschaltung & globaler CSV-Reset (System-Tab > *Globaler Reset*).
  Falls Button fehlt: Datei manuell lÃ¶schen (`mc_test_answers.csv`).

### Tests ausfÃ¼hren

```bash
pip install -r requirements.txt
# Haupt-App Tests (empfohlen):
PYTHONPATH=. pytest mc_test_app/tests -q
```

Hinweise:

- Haupttests: `mc_test_app/tests` (Core, Edge, Storage, UI).
- Deaktivierter UI-Test: `test_sidebar_leaderboard.py` (Skip).
- Legacy Root-`tests/` ggf. ignorieren.

### CI / QualitÃ¤t

- Automatische Tests via GitHub Actions.
- Schutz gegen fehlerhafte CSV-Zeilen.
- Retry-Logik bei Schreibfehlern.

---

## ğŸ¨ Accessibility & UX

- **Optionen:** Hoher Kontrast, groÃŸe Schrift, reduzierte Animationen.
- **Navigation:** Sticky Progress-Bar, Live-Countdown, Review-Modus.
- **Feedback:** Motivationales Design, ErklÃ¤rungen zu jeder Frage.

---

## ğŸ› Troubleshooting

### HÃ¤ufige Probleme

- **App startet nicht:** PrÃ¼fe Python-Version und AbhÃ¤ngigkeiten
  (`pip install -r requirements.txt`).
- **Fragen laden nicht:** Stelle sicher, dass `questions.json`
  vorhanden und gÃ¼ltig ist.
- **CSV-Fehler:** LÃ¶sche `mc_test_answers.csv` und starte neu (Daten gehen verloren).
- **Admin-Zugang fehlt:** PrÃ¼fe `.env` oder `secrets.toml` auf korrekte Werte.
- **Zeitlimit Ã¼berschritten:** Test ohne Zeitdruck neu starten (Pseudonym Ã¤ndern).

### Logs prÃ¼fen

- Streamlit-Logs: In der Konsole bei `streamlit run`.
- CSV-Logs: Ã–ffne `mc_test_answers.csv` mit Excel/Pandas.

### Hilfe

- Ã–ffne ein Issue auf GitHub oder kontaktiere den Entwickler.

---

## ğŸš€ Erweiterungsideen

- **Dynamische Fragen:** YAML-Quellen oder Rotation.
- **Mehrsprachigkeit:** Englische Ãœbersetzung.
- **Erweiterte Analyse:** ML-basierte Schwierigkeitsanalyse.
- **Integration:** Mit Jupyter fÃ¼r Datenanalyse kombinieren.

---

## ğŸ“ Changelog

- **2025-09-22:** Scoring Ã¼berarbeitet (Abzug = volle Gewichtung), README restrukturiert (Feature-Tabelle, Scoring-Abschnitt ergÃ¤nzt).
- **2025-09-22:** AufrÃ¤umarbeiten: Entfernte veraltete "Highscore"-Texte,
  README aktualisiert (vereinheitlichte Admin-Bereich Beschreibung,
  klare Trennung Ã¶ffentliche Ansicht vs. Admin-Tabs).
- **2025-09-21:** Module `leaderboard.py`, `review.py`; Analyse-/Glossar-Tabs;
  System-KPIs; Itemanalyse (p, r_pb, Distraktor %, Verteilungen);
  DEV-Fallback; hÃ¤rteres Admin-Auth; UI: Rang-Icons (ğŸ¥‡ğŸ¥ˆğŸ¥‰) fÃ¼r Top 3;
  leerer minimaler Leaderboard-Placeholder.
- **2025-09-20:** Scoring modularisiert (`scoring.py`), CI-Workflow
  (`mc_test_app_ci.yml`) ergÃ¤nzt, Modularchitektur dokumentiert.
- **2025-09-19:** README optimiert (Struktur, Klarheit, Troubleshooting hinzugefÃ¼gt).
- **2025-08-16:** Tests und README aktualisiert; Privacy-Ã„nderungen.
- **FrÃ¼her:** Grundfunktionen, Docker-UnterstÃ¼tzung.

---

## ğŸ¤ Contributing

BeitrÃ¤ge willkommen! Forke das Repo, erstelle einen Branch und Ã¶ffne einen Pull Request.
FÃ¼r grÃ¶ÃŸere Ã„nderungen: Issue erstellen.

**Letzte Aktualisierung:** 2025-09-21
