# ğŸ“ MC-Test Streamlit App

[![CI](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml)

Eine interaktive Multiple-Choice-Lern- und Selbsttest-App fÃ¼r Kursteilnehmer.
Bietet schnelles Feedback, Fortschrittsverfolgung und aggregierte Ergebnisse
fÃ¼r Data Science-Themen.

---

## ğŸš€ Ãœbersicht

Diese App ist ein vollstÃ¤ndiger MC-Test fÃ¼r Data Analytics, entwickelt mit Streamlit.
Sie ermÃ¶glicht anonyme Tests mit Pseudonymen, zufÃ¤lliger Fragenreihenfolge und Zeitlimit.
Perfekt fÃ¼r Bildungsumgebungen oder Selbstlernphasen.

### Hauptfunktionen

- **Benutzerverwaltung:** Anmeldung mit Pseudonym; Fortschritt wird gespeichert.
- **TestdurchfÃ¼hrung:** 100 zufÃ¤llig gemischte Fragen aus JSON-Datei,
  mit ErklÃ¤rungen und Review-Modus.
- **Zeitmanagement:** 60-Minuten-Limit mit Countdown und Warnungen.
- **Feedback & Analyse:** Motivationales Feedback, Leaderboard (Top 5),
  Admin-Bereich fÃ¼r Logs.
- **Datenschutz:** SHA-256-Hashing fÃ¼r AnonymitÃ¤t; lokale Speicherung.
- **ZusÃ¤tze:** Dark-Mode, Accessibility-Optionen, CSV-Exporte, Docker-UnterstÃ¼tzung.

---

## ğŸ“‹ Voraussetzungen

- **Python:** Version 3.8 oder hÃ¶her.
- **AbhÃ¤ngigkeiten:** Installiere via `pip install -r requirements.txt`.
- **Optionale Tools:** Docker fÃ¼r Container-Deployment; Git fÃ¼r Versionierung.

---

## ğŸ› ï¸ Installation und Start

### Lokaler Start (Empfohlen fÃ¼r Entwicklung)

1. Klone das Repository oder navigiere zum `mc_test_app/`-Ordner.
2. Installiere AbhÃ¤ngigkeiten:

   ```bash
   pip install -r requirements.txt
   ```

3. Starte die App:

   ```bash
   streamlit run mc_test_app.py
   ```

4. Ã–ffne [http://localhost:8501](http://localhost:8501) im Browser.

### Docker-Start

```bash
docker compose up -d streamlit-slim
```

FÃ¼r den vollen Stack (mit Jupyter, MLflow):

```bash
docker compose up -d
```

### Deployment (z.B. Streamlit Cloud)

1. Pushe nur den `mc_test_app/`-Ordner in ein separates Repo.
2. Verwende `git subtree` fÃ¼r saubere Trennung:

   ```bash
   git subtree push --prefix mc_test_app github main
   ```

3. Deploye auf Streamlit Cloud oder Ã¤hnlichen Plattformen.

---

## âš™ï¸ Konfiguration

### Umgebungsvariablen (`.env`-Datei)

Erstelle eine `.env`-Datei basierend auf `.env.example`:

```env
MC_TEST_ADMIN_USER=dein_admin_pseudonym  # Optional: BeschrÃ¤nkt Admin-Zugang
MC_TEST_ADMIN_KEY=dein_geheimes_passwort  # Erforderlich fÃ¼r Admin-Features
MC_TEST_MIN_SECONDS_BETWEEN=5  # Optional: Mindestsekunden zwischen Antworten
```

- **Admin-Zugang:** Ohne `MC_TEST_ADMIN_KEY` reicht ein beliebiges Passwort;
  mit Key muss es exakt passen.
- **Rate-Limiting:** Verhindert Spam; Standard: 0 (kein Limit).

### Streamlit-Secrets (`.streamlit/secrets.toml`)

FÃ¼r Produktion:

```toml
MC_TEST_ADMIN_USER = "admin"
MC_TEST_ADMIN_KEY = "secret123"
MC_TEST_MIN_SECONDS_BETWEEN = 5
```

### Datenpersistenz (CSV)

- **Datei:** `mc_test_answers.csv` (wird automatisch erstellt).
- **Schema (ab August 2025):**

  ```csv
  user_id_hash,user_id_display,user_id_plain,frage_nr,frage,antwort,richtig,zeit
  ```

- **Felder:**

  - `user_id_hash`: SHA-256-Hash des Pseudonyms (fÃ¼r AnonymitÃ¤t).
  - `user_id_display`: GekÃ¼rzter Hash (z.B. erste 10 Zeichen).
  - `user_id_plain`: Eingetragenes Pseudonym (fÃ¼r Leaderboard).
  - `frage_nr`: Fragenummer.
  - `frage`: VollstÃ¤ndiger Fragetext.
  - `antwort`: AusgewÃ¤hlte Option.
  - `richtig`: 1 (richtig) oder -1 (falsch).
  - `zeit`: ISO8601-Zeitstempel.

- **Eigenschaften:** Append-only, Pandas-kompatibel, leicht zu sichern.

---

## ğŸ“ Projektstruktur

```
mc_test_app/
â”œâ”€â”€ README.md                 # Diese Dokumentation
â”œâ”€â”€ mc_test_app.py            # Hauptapp (UI + kombinierte Logik â€“ wird schrittweise entschlackt)
â”œâ”€â”€ core.py                   # Speicher/Hash/CSV-Basisfunktionen
â”œâ”€â”€ scoring.py                # (Neu) Zentrale Score-/Leaderboard-Berechnung (Top-5 Abbildung)
â”œâ”€â”€ questions.json            # Fragenkatalog (JSON)
â”œâ”€â”€ requirements.txt          # AbhÃ¤ngigkeiten
â”œâ”€â”€ mc_test_answers.csv       # Antwort-Logs (auto-generiert)
â”œâ”€â”€ .env / .env.example       # ENV-Konfiguration
â”œâ”€â”€ __init__.py               # Paket-Marker
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json     # Dev-Container-Konfiguration
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ mc_test_app_ci.yml # Subtree-spezifischer CI-Workflow
â”œâ”€â”€ .streamlit/
â”‚   â”œâ”€â”€ config.toml           # Streamlit-Konfiguration
â”‚   â””â”€â”€ secrets.toml          # Secrets (fÃ¼r Produktion)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_core.py          # Kern-/App-Tests (Import-fallback)
â”‚   â”œâ”€â”€ test_edge_cases.py    # Edge Cases (Duplicate Guard, Leaderboard leer usw.)
â”‚   â”œâ”€â”€ test_storage.py       # File-Locking & Parallel-Append
â”‚   â””â”€â”€ test_ui.py            # UI-Sanity via streamlit.testing
â””â”€â”€ __pycache__/              # App-Cache
```

### Modularisierungsstand (Stand 2025â€‘09â€‘21)

| Modul | Zweck | Status |
|-------|-------|--------|
| `core.py` | CSV-Persistenz, Locking, Hashing, Fragenladen | Stabil |
| `scoring.py` | Punktestand, Prozent, abstrahiertes Leaderboard | Aktiv |
| `leaderboard.py` | Aggregationen, Leaderboard, Log-Ansicht (Admin) | Aktiv |
| `review.py` | Itemanalyse, Admin-Panel (4 Tabs) | Aktiv |
| `mc_test_app.py` | UI-Orchestrierung + Wrapper | Schlank |
| `gamification.py` | Badges, Streak, Motivation | Geplant |

Backward Compatibility: Wrapper-Funktionen im Hauptmodul behalten alte Namen
(`calculate_leaderboard`, `display_admin_panel` etc.), damit bestehende Tests
& externe Automationen nicht brechen.

### Warum Auslagerung?

- Geringere KomplexitÃ¤t (maintainable, testbar, klarere Verantwortlichkeiten)
- Saubere Trennung: UI vs. Analyse-/Aggregationslogik
- Wiederverwendbarkeit (spÃ¤ter ggf. Headless-Auswertung / API / Batch Reports)
- Besseres Onboarding neuer Contributor (kleinere Module)

### Neue / Erweiterte Funktionen seit Modularisierung

| Bereich | Ã„nderung | Nutzen |
|---------|----------|-------|
| Admin Auth | USER + KEY Pflicht (konstante Zeit) | Schutz |
| DEV Fallback | Auto-Credentials bei fehlender ENV | Schnelles Testen |
| Itemanalyse | p, r_pb, QualitÃ¤tslabel | Transparenz |
| Distraktor-Analyse | Dominanter Distraktor %, hÃ¤ufigste falsche | Diagnose |
| Detail-Ansicht | Verlauf + Verteilung pro Option | Item-Diagnose |
| System-Metriken | Nutzer, aktiv <10m, Ã˜ Antworten, Accuracy | Monitoring |
| Export-Tab | CSV-Download + Spaltenliste | Weiterverarb. |
| Glossar-Tab | Definitionen + Formeln | Kontext |
| Struktur | Module + Wrapper | Klarheit |

Geplante ErgÃ¤nzung: Auslagerung von Streak/Badges/Motivationslogik nach `gamification.py`.

### Admin-Panel (Tabs)

| Tab | Inhalt |
|-----|--------|
| ğŸ“Š Analyse | p, r_pb, Distraktor %, Details |
| ğŸ“¤ Export | Download des Antwort-Logs (`mc_test_answers.csv`) |
| ğŸ›  System | Laufende Nutzungs-/Systemmetriken (Nutzer, AktivitÃ¤t, Accuracy) |
| ğŸ“š Glossar | Kennzahlen & Formeln |

Glossar-Formeln: p, r_pb (vereinfachte Form), Dominanter Distraktor %.
Hinweis: Kleine Stichproben (<20) â†’ vorsichtige Interpretation.

### Integration der modularen Funktionen

`mc_test_app.py` delegiert via Wrapper an `scoring`, `leaderboard`, `review`.
Fehlschlagende Importe (Spezialumgebung) aktivieren Fallbacks.

---

## ğŸ”’ Datenschutz & Sicherheit

- **AnonymitÃ¤t:** Pseudonyme werden gehasht; nur Admins sehen Plaintext-Pseudonyme.
- **Lokale Speicherung:** Keine externen Server; Daten bleiben auf dem GerÃ¤t.
- **Admin-Schutz:** GeschÃ¼tzt durch ENV-Variablen; kein Zugriff ohne Key.
- **Rate-Limiting:** Verhindert Missbrauch (konfigurierbar).
- **Backup:** Sichere die CSV regelmÃ¤ÃŸig (z.B. via Git oder Cron).

**Hinweis:** Bei sensiblen Daten teste in isolierter Umgebung.

---

## ğŸ› ï¸ Admin & Wartung

### Admin-Bereich

- Zugang: Sidebar > Management > Key eingeben.
- Funktionen: Leaderboard anzeigen, Scoring-Modus Ã¤ndern,
  alle Daten lÃ¶schen (mit BestÃ¤tigung).
- CSV-Reset: LÃ¶sche `mc_test_answers.csv` manuell (wird neu erstellt).

### Tests ausfÃ¼hren

```bash
pip install -r requirements.txt
PYTHONPATH=. pytest tests/ -q
```

### CI / QualitÃ¤t

- Automatische Tests via GitHub Actions.
- Schutz gegen fehlerhafte CSV-Zeilen.
- Retry-Logik bei Schreibfehlern.

---

## ğŸ¨ Accessibility & UX

- **Optionen:** Hoher Kontrast, groÃŸe Schrift, reduzierte Animationen.
- **Navigation:** Sticky Progress-Bar, Live-Countdown, Review-Modus.
- **Feedback:** Motivationales Design, ErklÃ¤rungen zu jeder Frage.

---

## ğŸ› Troubleshooting

### HÃ¤ufige Probleme

- **App startet nicht:** PrÃ¼fe Python-Version und AbhÃ¤ngigkeiten
  (`pip install -r requirements.txt`).
- **Fragen laden nicht:** Stelle sicher, dass `questions.json`
  vorhanden und gÃ¼ltig ist.
- **CSV-Fehler:** LÃ¶sche `mc_test_answers.csv` und starte neu (Daten gehen verloren).
- **Admin-Zugang fehlt:** PrÃ¼fe `.env` oder `secrets.toml` auf korrekte Werte.
- **Zeitlimit Ã¼berschritten:** Test ohne Zeitdruck neu starten (Pseudonym Ã¤ndern).

### Logs prÃ¼fen

- Streamlit-Logs: In der Konsole bei `streamlit run`.
- CSV-Logs: Ã–ffne `mc_test_answers.csv` mit Excel/Pandas.

### Hilfe

- Ã–ffne ein Issue auf GitHub oder kontaktiere den Entwickler.

---

## ğŸš€ Erweiterungsideen

- **Dynamische Fragen:** YAML-Quellen oder Rotation.
- **Mehrsprachigkeit:** Englische Ãœbersetzung.
- **Erweiterte Analyse:** ML-basierte Schwierigkeitsanalyse.
- **Integration:** Mit Jupyter fÃ¼r Datenanalyse kombinieren.

---

## ğŸ“ Changelog

- **2025-09-21:** Module `leaderboard.py`, `review.py`; Admin-Panel Tabs; neue
Metriken (TrennschÃ¤rfe, p, Distraktor %, Verteilung); System-KPIs; Glossar
mit Formeln; DEV-Fallback; hÃ¤rteres Admin-Auth.
- **2025-09-20:** Scoring modularisiert (`scoring.py`), CI-Workflow
(`mc_test_app_ci.yml`) ergÃ¤nzt, Modularchitektur dokumentiert.
- **2025-09-19:** README optimiert (Struktur, Klarheit, Troubleshooting hinzugefÃ¼gt).
- **2025-08-16:** Tests und README aktualisiert; Privacy-Ã„nderungen.
- **FrÃ¼her:** Grundfunktionen, Docker-UnterstÃ¼tzung.

---

## ğŸ¤ Contributing

BeitrÃ¤ge willkommen! Forke das Repo, erstelle einen Branch und Ã¶ffne einen Pull Request.
FÃ¼r grÃ¶ÃŸere Ã„nderungen: Issue erstellen.

**Letzte Aktualisierung:** 2025-09-21
