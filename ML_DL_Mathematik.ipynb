{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ea869f",
   "metadata": {},
   "source": [
    "# ML/DL Mathematik – kompakter Spickzettel\n",
    "\n",
    "KaTeX/LaTeX-Formeln für zentrale Konzepte mit kurzen Demos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc241869",
   "metadata": {},
   "source": [
    "## Lineare Regression & MSE\n",
    "\n",
    "Loss: $$\\mathcal{L}(w) = \f",
    "rac{1}{N}\\sum_{i=1}^N (y_i - \\hat y_i)^2, \\quad \\hat y = Xw$$\n",
    "Gradient: $$\n",
    "abla_w \\mathcal{L} = -\f",
    "rac{2}{N} X^\top (y - Xw)$$\n",
    "Closed Form: $$w = (X^\top X)^{-1} X^\top y$$\n",
    "Regularisierung (Ridge): $$\\mathcal{L}_{ridge} = \text{MSE} + \\lambda \\|w\\|_2^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f869a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(200, 1)\n",
    "y = 3 * X[:,0] + 0.5 + 0.3 * np.random.randn(200)\n",
    "Xb = np.c_[np.ones_like(X), X]\n",
    "\n",
    "w = np.zeros(2)\n",
    "alpha = 0.1\n",
    "for _ in range(200):\n",
    "    y_hat = Xb @ w\n",
    "    grad = -(2/len(Xb)) * Xb.T @ (y - y_hat)\n",
    "    w -= alpha * grad\n",
    "w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f9c10",
   "metadata": {},
   "source": [
    "## Logistische Regression & Cross-Entropy\n",
    "\n",
    "Sigmoid: $$\\sigma(z) = \f",
    "rac{1}{1 + e^{-z}}$$\n",
    "Vorhersage: $$\\hat y = \\sigma(Xw)$$\n",
    "Loss: $$\\mathcal{L} = -\f",
    "rac{1}{N}\\sum_i \big[y_i \\log \\hat y_i + (1-y_i) \\log(1-\\hat y_i)\big]$$\n",
    "Gradient: $$\n",
    "abla_w \\mathcal{L} = \f",
    "rac{1}{N} X^\top (\\hat y - y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ced69",
   "metadata": {},
   "source": [
    "## Optimierung\n",
    "\n",
    "- Gradient Descent: $$w_{t+1} = w_t - \\eta \n",
    "abla_w \\mathcal{L}$$\n",
    "- Momentum: $$v_{t+1} = \beta v_t + (1-\beta) \n",
    "abla_w \\mathcal{L}, \\quad w_{t+1} = w_t - \\eta v_{t+1}$$\n",
    "- Adam (vereinfacht): $$m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t, \\ v_t=\beta_2 v_{t-1} + (1-\beta_2)g_t^2, \\ w_{t+1} = w_t - \\eta \f",
    "rac{m_t}{\\sqrt{v_t}+\\epsilon}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15915d5",
   "metadata": {},
   "source": [
    "## Regularisierung\n",
    "\n",
    "- L2 (Ridge): $$\\lambda \\|w\\|_2^2$$\n",
    "- L1 (Lasso): $$\\lambda \\|w\\|_1$$\n",
    "- Dropout: zufälliges Nullsetzen von Neuronen im Training.\n",
    "- Early Stopping: Stop bei steigendem Val-Loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb710af5",
   "metadata": {},
   "source": [
    "## Entscheidungsbäume & Ensemble-Signale\n",
    "\n",
    "- Entropie: $$H = -\\sum_k p_k \\log p_k$$\n",
    "- Gini: $$G = 1 - \\sum_k p_k^2$$\n",
    "- Informationsgewinn: $$IG = H(parent) - \\sum_j \f",
    "rac{N_j}{N} H(j)$$\n",
    "- Bagging/Boosting: viele schwache Modelle, gemittelt/gewichtet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c282",
   "metadata": {},
   "source": [
    "## Bias-Variance\n",
    "\n",
    "Bias = systematischer Fehler (Unteranpassung), Varianz = Sensitivität auf Rauschen (Überanpassung). Regularisierung/mehr Daten/Ensembles balancieren Bias/Varianz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce0c27",
   "metadata": {},
   "source": [
    "## Deep Learning Basics\n",
    "\n",
    "Dense-Forward: $$h = f(Wx + b)$$\n",
    "Backprop (vereinfacht): $$\f",
    "rac{\\partial L}{\\partial W} = \\delta x^T, \\ \\delta = \f",
    "rac{\\partial L}{\\partial h} \\cdot f'(z)$$\n",
    "Softmax: $$\text{softmax}(z_i) = \f",
    "rac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "Init: Xavier/Glorot (tanh/sigmoid), He (ReLU/GELU). BatchNorm: $$\\hat x = \f",
    "rac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}, \\ y = \\gamma \\hat x + \beta$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48cc15",
   "metadata": {},
   "source": [
    "## CNNs: Shapes\n",
    "\n",
    "Output: $$\text{out} = \\left\\lfloor \f",
    "rac{n + 2p - k}{s} \r",
    "ight\r",
    "floor + 1$$ (Kernel k, Stride s, Padding p). Pooling analog. Transfer Learning: Pretrained Backbone + kleiner Head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdd2c5",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "$$\text{Att}(Q,K,V) = \text{softmax}\\left(\f",
    "rac{QK^\top}{\\sqrt{d_k}}\r",
    "ight) V$$\n",
    "Multi-Head: mehrere Projektionen von Q/K/V, concat, linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6cf35",
   "metadata": {},
   "source": [
    "## Metriken\n",
    "\n",
    "- Klassifikation: Accuracy, Precision/Recall/F1, ROC-AUC, PR-AUC, Confusion Matrix.\n",
    "- Regression: MAE, MSE/RMSE, R².\n",
    "- Calibration: ECE/Brier.\n",
    "- Monitoring: Drift (Feature/Prediction), Latenz, Fehlerraten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = np.array([2.0, 1.0, 0.1])\n",
    "softmax = np.exp(z) / np.exp(z).sum()\n",
    "softmax\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}