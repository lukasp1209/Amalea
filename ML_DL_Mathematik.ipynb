{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ea869f",
   "metadata": {},
   "source": [
    "# ML/DL Mathematik – kompakter Spickzettel\n",
    "\n",
    "KaTeX/LaTeX-Formeln für zentrale Konzepte mit kurzen Demos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc241869",
   "metadata": {},
   "source": [
    "## Lineare Regression & MSE\n",
    "\n",
    "\n",
    "Loss: $$\\mathcal{L}(w) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat y_i)^2, \\quad \\hat y = Xw$$\n",
    "\n",
    "Gradient: $$\\nabla_w \\mathcal{L} = -\\frac{2}{N} X^\\top (y - Xw)$$\n",
    "\n",
    "Closed Form: $$w = (X^\\top X)^{-1} X^\\top y$$\n",
    "\n",
    "Regularisierung (Ridge): $$\\mathcal{L}_{ridge} = \\text{MSE} + \\lambda \\|w\\|_2^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7ee0e",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Loss (MSE):** Misst, wie weit die Vorhersagen $$\\hat y$$ von den echten Werten $$y$$ entfernt sind. Je kleiner, desto besser passt das Modell.\n",
    "- **Gradient:** Zeigt, wie man die Gewichte $$w$$ ändern muss, um den Fehler zu verringern. Wird für das Training per \"Gradient Descent\" genutzt.\n",
    "- **Closed Form:** Die optimale Lösung für $$w$$, wenn die Daten und das Modell einfach genug sind. Man muss nicht iterativ lernen.\n",
    "- **Ridge-Regularisierung:** Verhindert, dass die Gewichte zu groß werden (Überanpassung). $$\\lambda$$ steuert die Stärke der Bestrafung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f869a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46011025, 3.0209476 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(200, 1)\n",
    "y = 3 * X[:,0] + 0.5 + 0.3 * np.random.randn(200)\n",
    "Xb = np.c_[np.ones_like(X), X]\n",
    "\n",
    "w = np.zeros(2)\n",
    "alpha = 0.1\n",
    "for _ in range(200):\n",
    "    y_hat = Xb @ w\n",
    "    grad = -(2/len(Xb)) * Xb.T @ (y - y_hat)\n",
    "    w -= alpha * grad\n",
    "w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df53ccb",
   "metadata": {},
   "source": [
    "### Demo: Gradient Descent für Lineare Regression\n",
    "\n",
    "In diesem Beispiel werden die Gewichte $$w$$ mit Gradient Descent gelernt. Das Ziel: Die Gerade soll möglichst gut zu den Daten passen.\n",
    "\n",
    "- **X**: Eingabewerte (Merkmale)\n",
    "- **y**: Zielwerte (Labels)\n",
    "- **w**: Gewichte der Geraden\n",
    "- **alpha**: Lernrate (wie groß die Schritte sind)\n",
    "- **grad**: Richtung, in die $$w$$ angepasst werden muss\n",
    "\n",
    "Nach 200 Schritten ist $$w$$ so gewählt, dass der Fehler möglichst klein ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f9c10",
   "metadata": {},
   "source": [
    "## Logistische Regression & Cross-Entropy\n",
    "\n",
    "\n",
    "Sigmoid: $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Vorhersage: $$\\hat y = \\sigma(Xw)$$\n",
    "\n",
    "Loss: $$\\mathcal{L} = -\\frac{1}{N}\\sum_i \\big[y_i \\log \\hat y_i + (1-y_i) \\log(1-\\hat y_i)\\big]$$\n",
    "\n",
    "Gradient: $$\\nabla_w \\mathcal{L} = \\frac{1}{N} X^\\top (\\hat y - y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db369e43",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Sigmoid:** Wandelt beliebige Zahlen in Werte zwischen 0 und 1 um – ideal für Wahrscheinlichkeiten.\n",
    "- **Vorhersage:** Das Modell sagt die Wahrscheinlichkeit für Klasse 1 voraus.\n",
    "- **Loss (Cross-Entropy):** Bestraft falsche Vorhersagen besonders stark. Je kleiner, desto besser.\n",
    "- **Gradient:** Zeigt, wie die Gewichte $$w$$ angepasst werden müssen, damit die Vorhersagen besser werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ced69",
   "metadata": {},
   "source": [
    "## Optimierung\n",
    "\n",
    "\n",
    "- Gradient Descent: $$w_{t+1} = w_t - \\eta \\nabla_w \\mathcal{L}$$\n",
    "\n",
    "- Momentum: $$v_{t+1} = \\beta v_t + (1-\\beta) \\nabla_w \\mathcal{L}, \\quad w_{t+1} = w_t - \\eta v_{t+1}$$\n",
    "\n",
    "- Adam (vereinfacht): $$m_t=\\beta_1 m_{t-1} + (1-\\beta_1)g_t, \\ v_t=\\beta_2 v_{t-1} + (1-\\beta_2)g_t^2, \\ w_{t+1} = w_t - \\eta \\frac{m_t}{\\sqrt{v_t}+\\epsilon}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf65acb",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Gradient Descent:** Das Modell lernt, indem es die Gewichte Schritt für Schritt in Richtung des geringeren Fehlers anpasst.\n",
    "- **Momentum:** Sorgt dafür, dass das Modell nicht in kleinen Tälern stecken bleibt und schneller lernt.\n",
    "- **Adam:** Ein fortschrittlicher Algorithmus, der das Lernen noch robuster und schneller macht, indem er verschiedene Lernraten für jede Richtung nutzt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15915d5",
   "metadata": {},
   "source": [
    "## Regularisierung\n",
    "\n",
    "- L2 (Ridge): $$\\lambda \\|w\\|_2^2$$\n",
    "- L1 (Lasso): $$\\lambda \\|w\\|_1$$\n",
    "- Dropout: zufälliges Nullsetzen von Neuronen im Training.\n",
    "- Early Stopping: Stop bei steigendem Val-Loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32426dbd",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **L2 (Ridge):** Bestraft große Gewichte, damit das Modell nicht zu stark auf einzelne Datenpunkte reagiert.\n",
    "- **L1 (Lasso):** Sorgt dafür, dass manche Gewichte ganz auf Null gesetzt werden – das Modell wird sparsamer.\n",
    "- **Dropout:** Hilft, Überanpassung zu vermeiden, indem zufällig Teile des Modells beim Training ignoriert werden.\n",
    "- **Early Stopping:** Stoppt das Training, bevor das Modell anfängt, sich zu sehr an die Trainingsdaten zu klammern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb710af5",
   "metadata": {},
   "source": [
    "## Entscheidungsbäume & Ensemble-Signale\n",
    "\n",
    "\n",
    "- Entropie: $$H = -\\sum_k p_k \\log p_k$$\n",
    "\n",
    "- Gini: $$G = 1 - \\sum_k p_k^2$$\n",
    "\n",
    "- Informationsgewinn: $$IG = H(\\text{parent}) - \\sum_j \\frac{N_j}{N} H(j)$$\n",
    "\n",
    "- Bagging/Boosting: viele schwache Modelle, gemittelt/gewichtet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a65da",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Entropie:** Misst, wie \"gemischt\" die Klassen sind. Je höher, desto unsicherer die Entscheidung.\n",
    "- **Gini:** Alternative zu Entropie, misst ebenfalls die Reinheit eines Knotens im Baum.\n",
    "- **Informationsgewinn:** Zeigt, wie viel besser ein Split die Daten trennt. Je höher, desto besser.\n",
    "- **Bagging/Boosting:** Viele einfache Modelle werden kombiniert, um ein starkes Modell zu bauen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c282",
   "metadata": {},
   "source": [
    "## Bias-Variance\n",
    "\n",
    "Bias = systematischer Fehler (Unteranpassung), Varianz = Sensitivität auf Rauschen (Überanpassung). Regularisierung/mehr Daten/Ensembles balancieren Bias/Varianz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a122014",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Bias:** Das Modell ist zu einfach und verpasst wichtige Muster (Unteranpassung).\n",
    "- **Varianz:** Das Modell ist zu komplex und reagiert zu stark auf Zufall (Überanpassung).\n",
    "- **Lösung:** Mit Regularisierung, mehr Daten oder Ensemble-Methoden kann man Bias und Varianz ausbalancieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce0c27",
   "metadata": {},
   "source": [
    "## Deep Learning Basics\n",
    "\n",
    "\n",
    "Dense-Forward: $$h = f(Wx + b)$$\n",
    "\n",
    "Backprop (vereinfacht): $$\\frac{\\partial L}{\\partial W} = \\delta x^T, \\ \\delta = \\frac{\\partial L}{\\partial h} \\cdot f'(z)$$\n",
    "\n",
    "Softmax: $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "Init: Xavier/Glorot (tanh/sigmoid), He (ReLU/GELU). BatchNorm: $$\\hat x = \\frac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}, \\ y = \\gamma \\hat x + \\beta$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d25421",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Dense-Forward:** Ein Neuron berechnet eine gewichtete Summe der Eingaben und wendet eine Aktivierungsfunktion an.\n",
    "- **Backpropagation:** Zeigt, wie die Gewichte angepasst werden, damit der Fehler kleiner wird.\n",
    "- **Softmax:** Wandelt die Ausgaben in Wahrscheinlichkeiten um, sodass sie zusammen 1 ergeben.\n",
    "- **BatchNorm:** Normalisiert die Eingaben, damit das Training stabiler und schneller wird.\n",
    "- **Init:** Die Startwerte der Gewichte sind wichtig, damit das Modell gut lernen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48cc15",
   "metadata": {},
   "source": [
    "## CNNs: Shapes\n",
    "\n",
    "\n",
    "Output: $$\\text{out} = \\left\\lfloor \\frac{n + 2p - k}{s} \\right\\rfloor + 1$$ (Kernel k, Stride s, Padding p). Pooling analog. Transfer Learning: Pretrained Backbone + kleiner Head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6022a2ae",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **CNN Output-Shape:** Zeigt, wie groß das Ergebnis nach einer Faltung ist. Wichtig für die Architektur.\n",
    "- **Pooling:** Reduziert die Größe und hebt wichtige Merkmale hervor.\n",
    "- **Transfer Learning:** Man nutzt ein vortrainiertes Modell und passt nur den letzten Teil für die eigene Aufgabe an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdd2c5",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "\n",
    "$$\\text{Att}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Multi-Head: mehrere Projektionen von Q/K/V, concat, linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f23c5a",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Attention:** Das Modell lernt, auf die wichtigsten Teile der Eingabe zu achten. Die Formel zeigt, wie die Aufmerksamkeit berechnet wird.\n",
    "- **Multi-Head:** Mehrere \"Blicke\" auf die Daten gleichzeitig, damit das Modell verschiedene Muster erkennt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6cf35",
   "metadata": {},
   "source": [
    "## Metriken\n",
    "\n",
    "- Klassifikation: Accuracy, Precision/Recall/F1, ROC-AUC, PR-AUC, Confusion Matrix.\n",
    "- Regression: MAE, MSE/RMSE, R².\n",
    "- Calibration: ECE/Brier.\n",
    "- Monitoring: Drift (Feature/Prediction), Latenz, Fehlerraten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d489fd4",
   "metadata": {},
   "source": [
    "### Erklärungen zu den Formeln:\n",
    "\n",
    "- **Klassifikation:** Misst, wie gut das Modell zwischen Klassen unterscheidet (z.B. Katzen vs. Hunde).\n",
    "- **Regression:** Misst, wie nah die Vorhersagen an den echten Zahlen liegen.\n",
    "- **Calibration:** Zeigt, ob die Wahrscheinlichkeiten des Modells wirklich stimmen.\n",
    "- **Monitoring:** Überwacht, ob sich die Daten oder das Modell im Laufe der Zeit verändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba7ab40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65900114, 0.24243297, 0.09856589])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "z = np.array([2.0, 1.0, 0.1])\n",
    "softmax = np.exp(z) / np.exp(z).sum()\n",
    "softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8661c6",
   "metadata": {},
   "source": [
    "### Demo: Softmax für Wahrscheinlichkeiten\n",
    "\n",
    "Hier wird gezeigt, wie die Softmax-Funktion aus beliebigen Zahlen (z.B. Modell-Ausgaben) Wahrscheinlichkeiten macht:\n",
    "\n",
    "- **z**: Die Roh-Ausgaben des Modells (z.B. für drei Klassen)\n",
    "- **softmax**: Die Wahrscheinlichkeiten für jede Klasse, immer zwischen 0 und 1 und zusammen genau 1\n",
    "\n",
    "Das ist wichtig für Klassifikation, z.B. bei Bildern: Das Modell sagt, wie wahrscheinlich jede Klasse ist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
